{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95cf1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from timeit import default_timer as timer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import TextClassifierTrainDataset, TextClassifierInferenceDataset\n",
    "from dataloaders import get_train_loader, get_inference_loader\n",
    "from model import TransformerClassifier, DemographicDiscriminator, CombinedModel\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3716e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fpr(y_true, y_pred):\n",
    "    tot_neg = (y_true == False).sum()\n",
    "    fp      = ((y_true == False) & (y_pred == True)).sum()\n",
    "    # print(tot_neg, fp)\n",
    "    return fp/tot_neg\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_epoch(model, loss_fn, train_dataloader, optimizer, DEVICE):\n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "                \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        logits, _ = model(src)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(train_dataloader)}: loss {loss}', end='', flush=True)\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate(model, loss_fn, val_dataloader, idx_to_class, DEVICE):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    str_pred = []\n",
    "    str_true = []\n",
    "    for i, (src, tgt) in enumerate(val_dataloader):\n",
    "                \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "\n",
    "        logits, _ = model(src)\n",
    "\n",
    "        # token predictions\n",
    "        tgt_pred = torch.max(logits, dim=-1).indices  # (T, N)\n",
    "\n",
    "        # loop over examples in batch\n",
    "        for x in range(tgt_pred.shape[-1]):\n",
    "\n",
    "            tgt_x      = tgt[x]\n",
    "            tgt_pred_x = tgt_pred[x]\n",
    "\n",
    "            tgt_x_string      = idx_to_class[tgt_x.tolist()]\n",
    "            tgt_pred_x_string = idx_to_class[tgt_pred_x.tolist()]\n",
    "\n",
    "            str_true.append(tgt_x_string)\n",
    "            str_pred.append(tgt_pred_x_string)\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(val_dataloader)}: loss {loss}', end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return losses / len(val_dataloader), np.mean([int(te == pe) for te, pe in zip(str_true, str_pred)]), str_true, str_pred\n",
    "\n",
    "\n",
    "def evaluate_with_protected_attribute(model, loss_fn, val_dataloader, idx_to_class, idx_to_prot_attr, DEVICE):\n",
    "    model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    str_pred   = []\n",
    "    str_true   = []\n",
    "    prot_attrs = []\n",
    "    for i, (src, tgt, prot_attr) in enumerate(val_dataloader):\n",
    "                \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        prot_attr = prot_attr.to(DEVICE)\n",
    "\n",
    "        logits, _ = model(src)\n",
    "\n",
    "        # token predictions\n",
    "        tgt_pred = torch.max(logits, dim=-1).indices  # (T, N)\n",
    "\n",
    "        # loop over examples in batch\n",
    "        for x in range(tgt_pred.shape[-1]):\n",
    "\n",
    "            tgt_x       = tgt[x]\n",
    "            tgt_pred_x  = tgt_pred[x]\n",
    "            prot_attr_x = prot_attr[x]\n",
    "\n",
    "            tgt_x_string       = idx_to_class[tgt_x.tolist()]\n",
    "            tgt_pred_x_string  = idx_to_class[tgt_pred_x.tolist()]\n",
    "            prot_attr_x_string = idx_to_prot_attr[prot_attr_x.tolist()]\n",
    "            \n",
    "            str_true.append(tgt_x_string)\n",
    "            str_pred.append(tgt_pred_x_string)\n",
    "            prot_attrs.append(prot_attr_x_string)\n",
    "        \n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(val_dataloader)}: loss {loss}', end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return losses / len(val_dataloader), np.mean([int(te == pe) for te, pe in zip(str_true, str_pred)]), str_true, str_pred, prot_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac1de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10592, 3)\n",
      "(24783, 7)\n",
      "(1324, 4)\n",
      "(5072, 4)\n"
     ]
    }
   ],
   "source": [
    "# train data \n",
    "train_df = pd.read_csv('civility_data/train.tsv', sep='\\t')\n",
    "print(train_df.shape)\n",
    "\n",
    "# extra_data\n",
    "extra_df = pd.read_csv('civility_extra_data/labeled_data.tsv', sep='\\t')\n",
    "print(extra_df.shape)\n",
    "\n",
    "# dev data\n",
    "dev_df = pd.read_csv('civility_data/dev.tsv', sep='\\t')\n",
    "print(dev_df.shape)\n",
    "\n",
    "# demog dev: for FPR disparity\n",
    "demog_dev_df = pd.read_csv('civility_data/mini_demographic_dev.tsv', sep='\\t')\n",
    "demog_dev_df['label'] = 'NOT'\n",
    "print(demog_dev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa239ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>demographic</th>\n",
       "      <th>perspective_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>People make mistakes. It takes a good person to learn from them. #learning</td>\n",
       "      <td>White</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Only one on our road with power, but no cable or internet... Guess I can't have it all.</td>\n",
       "      <td>White</td>\n",
       "      <td>0.061435</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I love when baby's yawn I think it's so cute.</td>\n",
       "      <td>White</td>\n",
       "      <td>0.056817</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theres so many hoes now that i actually think about it..</td>\n",
       "      <td>White</td>\n",
       "      <td>0.503459</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Today is the day Adalynn Alexis will be here! I can't believe that 9 months ago I was the first to know and now it's finally here :)))</td>\n",
       "      <td>White</td>\n",
       "      <td>0.092183</td>\n",
       "      <td>NOT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                     text  \\\n",
       "0                                                              People make mistakes. It takes a good person to learn from them. #learning   \n",
       "1                                                 Only one on our road with power, but no cable or internet... Guess I can't have it all.   \n",
       "2                                                                                           I love when baby's yawn I think it's so cute.   \n",
       "3                                                                                theres so many hoes now that i actually think about it..   \n",
       "4  Today is the day Adalynn Alexis will be here! I can't believe that 9 months ago I was the first to know and now it's finally here :)))   \n",
       "\n",
       "  demographic  perspective_score label  \n",
       "0       White           0.041031   NOT  \n",
       "1       White           0.061435   NOT  \n",
       "2       White           0.056817   NOT  \n",
       "3       White           0.503459   NOT  \n",
       "4       White           0.092183   NOT  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39fda9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # train-val-test split\n",
    "# train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
    "# train_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "802d1497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = TextClassifierTrainDataset(\n",
    "    df = train_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    "    class_names = train_df['label'].unique().tolist(),\n",
    "    freq_threshold = 3,\n",
    "    max_size = None\n",
    ")\n",
    "val_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = dev_df, # val_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    ")\n",
    "demog_dev_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = demog_dev_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    "    protected_attribute_column = 'demographic',\n",
    "    protected_attribute_names = demog_dev_df['demographic'].unique().tolist(),\n",
    "    protected_attribute_source = 'self'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9abc8496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<SOS> @ user seems hard to believe that you stood next to a guy wearing those <UNK> and didn ' t <UNK> but i ' ll take your word for it <EOS>\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, cls = train_dataset[1]\n",
    "text_readable, cls_readable = train_dataset.get_readable_tokens(text, cls)\n",
    "' '.join(text_readable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc726c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataloader = get_train_loader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = 32\n",
    ")\n",
    "val_dataloader   = get_inference_loader(\n",
    "    dataset = val_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32\n",
    ")\n",
    "demog_dev_dataloader = get_inference_loader(\n",
    "    dataset = demog_dev_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32,\n",
    "    return_protected_attribute = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b06d639a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4574"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset.vocab.stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f484421c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10592"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e559f154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 1.8119335347432024\n",
      "middle 90% range: 0.0 6.0\n"
     ]
    }
   ],
   "source": [
    "# percentage of unknown tokens in an example\n",
    "\n",
    "train_dataloader_ = get_train_loader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = 1\n",
    ")\n",
    "\n",
    "UNK_IDX = train_dataset.vocab.stoi['<UNK>']\n",
    "num_unk_per_example = np.zeros(len(train_dataset))\n",
    "for i, (src, tgt) in enumerate(train_dataloader_):\n",
    "    src = src.cpu().detach().numpy().squeeze()\n",
    "    num_unk = (src == UNK_IDX).sum()\n",
    "    num_unk_per_example[i] = num_unk\n",
    "    \n",
    "num_unk_per_example.sort()\n",
    "unk_mean = num_unk_per_example.mean()\n",
    "unk_std  = num_unk_per_example.std()\n",
    "print('mean:', unk_mean)\n",
    "print('middle 90% range:', num_unk_per_example[int(0.05 * len(train_dataset))], num_unk_per_example[int(0.95 * len(train_dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b222868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters: 872194\n"
     ]
    }
   ],
   "source": [
    "# model parameters\n",
    "torch.manual_seed(0)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PAD_IDX = train_dataset.vocab.stoi['<PAD>']\n",
    "BOS_IDX = train_dataset.vocab.stoi['<SOS>']\n",
    "EOS_IDX = train_dataset.vocab.stoi['<EOS>']\n",
    "VOCAB_SIZE = len(train_dataset.vocab.stoi)\n",
    "EMB_SIZE = 64\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# initialize model\n",
    "transformer = TransformerClassifier(\n",
    "    num_encoder_layers = NUM_ENCODER_LAYERS,\n",
    "    emb_size = EMB_SIZE,\n",
    "    nhead = NHEAD,\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    dim_feedforward = FFN_HID_DIM,\n",
    "    dropout = DROPOUT,\n",
    "    return_feature_vector = True\n",
    ")\n",
    "for p in transformer.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "transformer = transformer.to(DEVICE)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "print('Number of model parameters:', count_parameters(transformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "607dc17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([117, 32]), torch.Size([32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (src, tgt) in enumerate(val_dataloader):\n",
    "    break\n",
    "    \n",
    "src.shape, tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79f0c42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   1,   1,  ...,   1,   1,   1],\n",
       "        [  4,   4,   3,  ...,   4,   4,   4],\n",
       "        [  5,   5, 138,  ...,   5,   5,   5],\n",
       "        ...,\n",
       "        [  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  0,   0,   0,  ...,   0,   0,   0],\n",
       "        [  0,   0,   0,  ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "901cbf8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 330 of 331: loss 0.5956400632858276\n",
      "Processing batch 40 of 42: loss 0.5480090379714966\n",
      "Processing batch 150 of 159: loss 0.5567930340766907\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 1, Train loss: 0.635, Train acc: 0.664, Epoch time = 7.193s\n",
      "Epoch: 1, Val loss  : 0.641, Val acc  : 0.661,   Val F1  : 0.793 Epoch time = 7.193s\n",
      "Epoch: 1, Demog loss: 0.536, Demog acc: 0.931, Demog F1: 0.964 Epoch time = 7.193s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.5934918522834778\n",
      "Processing batch 40 of 42: loss 0.5600728988647461\n",
      "Processing batch 150 of 159: loss 0.47144243121147156\n",
      "\n",
      "Epoch: 2, Train loss: 0.631, Train acc: 0.661, Epoch time = 7.124s\n",
      "Epoch: 2, Val loss  : 0.640, Val acc  : 0.657,   Val F1  : 0.785 Epoch time = 7.124s\n",
      "Epoch: 2, Demog loss: 0.466, Demog acc: 0.972, Demog F1: 0.986 Epoch time = 7.124s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.5925381779670715\n",
      "Processing batch 40 of 42: loss 0.6076177358627319\n",
      "Processing batch 150 of 159: loss 0.42660018801689154\n",
      "\n",
      "Epoch: 3, Train loss: 0.629, Train acc: 0.654, Epoch time = 7.307s\n",
      "Epoch: 3, Val loss  : 0.644, Val acc  : 0.637,   Val F1  : 0.763 Epoch time = 7.307s\n",
      "Epoch: 3, Demog loss: 0.423, Demog acc: 0.970, Demog F1: 0.985 Epoch time = 7.307s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.5858826637268066\n",
      "Processing batch 40 of 42: loss 0.5448035001754761\n",
      "Processing batch 150 of 159: loss 0.36584442853927616\n",
      "\n",
      "Epoch: 4, Train loss: 0.617, Train acc: 0.664, Epoch time = 7.179s\n",
      "Epoch: 4, Val loss  : 0.635, Val acc  : 0.654,   Val F1  : 0.780 Epoch time = 7.179s\n",
      "Epoch: 4, Demog loss: 0.365, Demog acc: 0.971, Demog F1: 0.985 Epoch time = 7.179s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.58001238107681274\n",
      "Processing batch 40 of 42: loss 0.6464834213256836\n",
      "Processing batch 150 of 159: loss 0.47382202744483954\n",
      "\n",
      "Epoch: 5, Train loss: 0.614, Train acc: 0.662, Epoch time = 7.197s\n",
      "Epoch: 5, Val loss  : 0.644, Val acc  : 0.624,   Val F1  : 0.738 Epoch time = 7.197s\n",
      "Epoch: 5, Demog loss: 0.467, Demog acc: 0.867, Demog F1: 0.929 Epoch time = 7.197s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.5633466839790344\n",
      "Processing batch 40 of 42: loss 0.7238014340400696\n",
      "Processing batch 150 of 159: loss 0.43925949931144714\n",
      "\n",
      "Epoch: 6, Train loss: 0.601, Train acc: 0.681, Epoch time = 7.198s\n",
      "Epoch: 6, Val loss  : 0.645, Val acc  : 0.633,   Val F1  : 0.727 Epoch time = 7.198s\n",
      "Epoch: 6, Demog loss: 0.444, Demog acc: 0.858, Demog F1: 0.923 Epoch time = 7.198s\n",
      "================================================================================================================\n",
      "5 epochs since last improved. Loading from checkpoint...\n",
      "Processing batch 330 of 331: loss 0.50254935026168824\n",
      "Processing batch 40 of 42: loss 0.5215638875961304\n",
      "Processing batch 150 of 159: loss 0.32248166203498843\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 7, Train loss: 0.528, Train acc: 0.744, Epoch time = 7.238s\n",
      "Epoch: 7, Val loss  : 0.585, Val acc  : 0.701,   Val F1  : 0.795 Epoch time = 7.238s\n",
      "Epoch: 7, Demog loss: 0.356, Demog acc: 0.891, Demog F1: 0.942 Epoch time = 7.238s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.49508172273635864\n",
      "Processing batch 40 of 42: loss 0.42537161707878113\n",
      "Processing batch 150 of 159: loss 0.26498940587043768\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 8, Train loss: 0.515, Train acc: 0.761, Epoch time = 7.523s\n",
      "Epoch: 8, Val loss  : 0.584, Val acc  : 0.738,   Val F1  : 0.829 Epoch time = 7.523s\n",
      "Epoch: 8, Demog loss: 0.390, Demog acc: 0.903, Demog F1: 0.949 Epoch time = 7.523s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.48568052053451545\n",
      "Processing batch 40 of 42: loss 0.44740578532218933\n",
      "Processing batch 150 of 159: loss 0.23889468610286713\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 9, Train loss: 0.500, Train acc: 0.770, Epoch time = 7.387s\n",
      "Epoch: 9, Val loss  : 0.574, Val acc  : 0.750,   Val F1  : 0.836 Epoch time = 7.387s\n",
      "Epoch: 9, Demog loss: 0.380, Demog acc: 0.911, Demog F1: 0.953 Epoch time = 7.387s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.44723099470138556\n",
      "Processing batch 40 of 42: loss 0.5088803768157959\n",
      "Processing batch 150 of 159: loss 0.28532591462135315\n",
      "\n",
      "Epoch: 10, Train loss: 0.471, Train acc: 0.789, Epoch time = 7.465s\n",
      "Epoch: 10, Val loss  : 0.562, Val acc  : 0.748,   Val F1  : 0.831 Epoch time = 7.465s\n",
      "Epoch: 10, Demog loss: 0.399, Demog acc: 0.894, Demog F1: 0.944 Epoch time = 7.465s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.40546223521232605\n",
      "Processing batch 40 of 42: loss 0.5047121047973633\n",
      "Processing batch 150 of 159: loss 0.30928027629852295\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 11, Train loss: 0.453, Train acc: 0.804, Epoch time = 7.224s\n",
      "Epoch: 11, Val loss  : 0.562, Val acc  : 0.757,   Val F1  : 0.835 Epoch time = 7.224s\n",
      "Epoch: 11, Demog loss: 0.388, Demog acc: 0.881, Demog F1: 0.937 Epoch time = 7.224s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.40000462532043457\n",
      "Processing batch 40 of 42: loss 0.4367237091064453\n",
      "Processing batch 150 of 159: loss 0.28268924355506897\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 12, Train loss: 0.449, Train acc: 0.807, Epoch time = 7.486s\n",
      "Epoch: 12, Val loss  : 0.560, Val acc  : 0.761,   Val F1  : 0.839 Epoch time = 7.486s\n",
      "Epoch: 12, Demog loss: 0.327, Demog acc: 0.887, Demog F1: 0.940 Epoch time = 7.486s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.40555626153945923\n",
      "Processing batch 40 of 42: loss 0.3960151970386505\n",
      "Processing batch 150 of 159: loss 0.21987852454185486\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 13, Train loss: 0.479, Train acc: 0.806, Epoch time = 7.272s\n",
      "Epoch: 13, Val loss  : 0.611, Val acc  : 0.765,   Val F1  : 0.843 Epoch time = 7.272s\n",
      "Epoch: 13, Demog loss: 0.295, Demog acc: 0.902, Demog F1: 0.949 Epoch time = 7.272s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.34901016950607336\n",
      "Processing batch 40 of 42: loss 0.4362820088863373\n",
      "Processing batch 150 of 159: loss 0.3087390959262848\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 14, Train loss: 0.432, Train acc: 0.824, Epoch time = 7.657s\n",
      "Epoch: 14, Val loss  : 0.576, Val acc  : 0.782,   Val F1  : 0.850 Epoch time = 7.657s\n",
      "Epoch: 14, Demog loss: 0.316, Demog acc: 0.882, Demog F1: 0.937 Epoch time = 7.657s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.34906169772148135\n",
      "Processing batch 40 of 42: loss 0.4052652418613434\n",
      "Processing batch 150 of 159: loss 0.32594141364097595\n",
      "\n",
      "Epoch: 15, Train loss: 0.453, Train acc: 0.824, Epoch time = 7.371s\n",
      "Epoch: 15, Val loss  : 0.609, Val acc  : 0.777,   Val F1  : 0.849 Epoch time = 7.371s\n",
      "Epoch: 15, Demog loss: 0.317, Demog acc: 0.880, Demog F1: 0.936 Epoch time = 7.371s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.33344733715057373\n",
      "Processing batch 40 of 42: loss 0.3777083158493042\n",
      "Processing batch 150 of 159: loss 0.250979810953140266\n",
      "\n",
      "Epoch: 16, Train loss: 0.450, Train acc: 0.825, Epoch time = 7.294s\n",
      "Epoch: 16, Val loss  : 0.616, Val acc  : 0.778,   Val F1  : 0.850 Epoch time = 7.294s\n",
      "Epoch: 16, Demog loss: 0.274, Demog acc: 0.894, Demog F1: 0.944 Epoch time = 7.294s\n",
      "================================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 330 of 331: loss 0.32631021738052371\n",
      "Processing batch 40 of 42: loss 0.3595489561557778\n",
      "Processing batch 150 of 159: loss 0.25945049524307252\n",
      "\n",
      "Epoch: 17, Train loss: 0.464, Train acc: 0.827, Epoch time = 7.378s\n",
      "Epoch: 17, Val loss  : 0.644, Val acc  : 0.778,   Val F1  : 0.849 Epoch time = 7.378s\n",
      "Epoch: 17, Demog loss: 0.276, Demog acc: 0.889, Demog F1: 0.941 Epoch time = 7.378s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.31173983216285706\n",
      "Processing batch 40 of 42: loss 0.3439386785030365\n",
      "Processing batch 150 of 159: loss 0.22327381372451782\n",
      "\n",
      "Epoch: 18, Train loss: 0.470, Train acc: 0.826, Epoch time = 7.419s\n",
      "Epoch: 18, Val loss  : 0.665, Val acc  : 0.779,   Val F1  : 0.851 Epoch time = 7.419s\n",
      "Epoch: 18, Demog loss: 0.264, Demog acc: 0.894, Demog F1: 0.944 Epoch time = 7.419s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: loss 0.30996477603912354\n",
      "Processing batch 40 of 42: loss 0.36006245017051697\n",
      "Processing batch 150 of 159: loss 0.22617807984352112\n",
      "\n",
      "Epoch: 19, Train loss: 0.475, Train acc: 0.827, Epoch time = 7.536s\n",
      "Epoch: 19, Val loss  : 0.682, Val acc  : 0.776,   Val F1  : 0.849 Epoch time = 7.536s\n",
      "Epoch: 19, Demog loss: 0.277, Demog acc: 0.895, Demog F1: 0.945 Epoch time = 7.536s\n",
      "================================================================================================================\n",
      "5 epochs since last improved. Loading from checkpoint...\n",
      "Processing batch 330 of 331: loss 0.29231122136116035\n",
      "Processing batch 40 of 42: loss 0.3632003962993622\n",
      "Processing batch 150 of 159: loss 0.22438220679759984\n",
      "\n",
      "Epoch: 20, Train loss: 0.463, Train acc: 0.832, Epoch time = 7.273s\n",
      "Epoch: 20, Val loss  : 0.678, Val acc  : 0.775,   Val F1  : 0.847 Epoch time = 7.273s\n",
      "Epoch: 20, Demog loss: 0.280, Demog acc: 0.896, Demog F1: 0.945 Epoch time = 7.273s\n",
      "================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "best_val_acc = -1\n",
    "best_transformer_state_dict = None\n",
    "epochs_since_last_improved = 0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "\n",
    "    # grab last best model if not improved for `patience` epochs\n",
    "    if epochs_since_last_improved >= PATIENCE:\n",
    "        print(f'{epochs_since_last_improved} epochs since last improved. Loading from checkpoint...')\n",
    "        transformer.load_state_dict(best_transformer_state_dict['model_state_dict'])\n",
    "        optimizer.load_state_dict(best_transformer_state_dict['optimizer_state_dict'])\n",
    "        epochs_since_last_improved = 0\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    # train one epoch\n",
    "    train_loss = train_epoch(\n",
    "        model = transformer, \n",
    "        loss_fn = loss_fn, \n",
    "        train_dataloader = train_dataloader, \n",
    "        optimizer = optimizer,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "\n",
    "    # evaluate train and val set at end of epoch\n",
    "    train_loss, train_acc, train_true, train_pred = evaluate(\n",
    "        model = transformer, \n",
    "        loss_fn = loss_fn, \n",
    "        val_dataloader = train_dataloader,\n",
    "        idx_to_class = train_dataset.idx_to_class,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "    val_loss, val_acc, val_true, val_pred = evaluate(\n",
    "        model = transformer, \n",
    "        loss_fn = loss_fn, \n",
    "        val_dataloader = val_dataloader,\n",
    "        idx_to_class = train_dataset.idx_to_class,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "    val_true_numeric = np.array([1 if x == 'OFF' else 0 for x in val_true])\n",
    "    val_pred_numeric = np.array([1 if x == 'OFF' else 0 for x in val_pred])\n",
    "    val_f1 = f1_score(1-val_true_numeric, 1-val_pred_numeric)\n",
    "    demog_loss, demog_acc, demog_true, demog_pred, prot_attrs = evaluate_with_protected_attribute(\n",
    "        model = transformer, \n",
    "        loss_fn = loss_fn, \n",
    "        val_dataloader = demog_dev_dataloader,\n",
    "        idx_to_class = train_dataset.idx_to_class,\n",
    "        idx_to_prot_attr = demog_dev_dataset.idx_to_prot_attr,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "    demog_true_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_true])\n",
    "    demog_pred_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_pred])\n",
    "    demog_f1 = f1_score(1-demog_true_numeric, 1-demog_pred_numeric)\n",
    "    \n",
    "    end_time = timer()\n",
    "\n",
    "    # checkpoint if train acc is better than last\n",
    "    if val_acc > best_val_acc:\n",
    "        print(f'New best model found! saving...')\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': transformer.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc\n",
    "        }\n",
    "        torch.save(state_dict, f'checkpoints/transformer_v1_{epoch}_valacc{val_acc}.model')\n",
    "        best_val_acc = val_acc\n",
    "        best_transformer_state_dict = state_dict\n",
    "        epochs_since_last_improved = 0\n",
    "    else:\n",
    "        epochs_since_last_improved += 1\n",
    "\n",
    "    print()\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Train acc: {train_acc:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print((f\"Epoch: {epoch}, Val loss  : {val_loss:.3f}, Val acc  : {val_acc:.3f},   Val F1  : {val_f1:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print((f\"Epoch: {epoch}, Demog loss: {demog_loss:.3f}, Demog acc: {demog_acc:.3f}, Demog F1: {demog_f1:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "           \n",
    "    print(\"================================================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8d06fa",
   "metadata": {},
   "source": [
    "# Evaluate on `demog_dev_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15485c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerClassifier(\n",
       "  (src_tok_emb): TokenEmbedding(\n",
       "    (embedding): Embedding(4574, 64)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "  (fc1_activation): Tanh()\n",
       "  (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "checkpoint = torch.load('checkpoints/transformer_v1_14_valacc0.7817220543806647.model', map_location=DEVICE)\n",
    "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "transformer.to(DEVICE)\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d86b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 40 of 42: loss 0.4362820088863373\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_true, val_pred = evaluate(\n",
    "    model = transformer, \n",
    "    loss_fn = loss_fn, \n",
    "    val_dataloader = val_dataloader,\n",
    "    idx_to_class = train_dataset.idx_to_class,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "val_true_numeric = np.array([1 if x == 'OFF' else 0 for x in val_true])\n",
    "val_pred_numeric = np.array([1 if x == 'OFF' else 0 for x in val_pred])\n",
    "val_f1 = f1_score(1-val_true_numeric, 1-val_pred_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319b7fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8501814411612235"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7cb055b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 150 of 159: loss 0.3087390959262848\n"
     ]
    }
   ],
   "source": [
    "demog_loss, demog_acc, demog_true, demog_pred, prot_attr = evaluate_with_protected_attribute(\n",
    "    model = transformer, \n",
    "    loss_fn = loss_fn, \n",
    "    val_dataloader = demog_dev_dataloader,\n",
    "    idx_to_class = train_dataset.idx_to_class,\n",
    "    idx_to_prot_attr = demog_dev_dataset.idx_to_prot_attr,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "demog_true_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_true])\n",
    "demog_pred_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_pred])\n",
    "demog_f1 = f1_score(1-demog_true_numeric, 1-demog_pred_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e41c7455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# report FPR for different demographic groups in demog_dev_df:\n",
    "demog_dev_df_pred = pd.DataFrame({\n",
    "    'is_offensive_true': demog_true_numeric,\n",
    "    'is_offensive_pred': demog_pred_numeric,\n",
    "    'demographic': prot_attr\n",
    "})\n",
    "demog_fpr  = {\n",
    "    demog: fpr(\n",
    "        y_true = demog_dev_df_pred[demog_dev_df_pred['demographic'] == demog]['is_offensive_true'],\n",
    "        y_pred = demog_dev_df_pred[demog_dev_df_pred['demographic'] == demog]['is_offensive_pred']\n",
    "    ) for demog in demog_dev_df_pred['demographic'].unique()\n",
    "}\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.title('FPR per demographic group')\n",
    "plt.bar(demog_fpr.keys(), demog_fpr.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5baa1884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.11216056670602124, 0.12835820895522387, 0.24096385542168675, 0.0])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_fpr.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6d7f217c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08538591971317144"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(list(demog_fpr.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "477beffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "demog_fpr_before = demog_fpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b81ddb",
   "metadata": {},
   "source": [
    "# Pre-train the adversarial discriminator on `demographic_train.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fba8598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 256])\n"
     ]
    }
   ],
   "source": [
    "# # filter data: remove examples with too many unknown words based on the main task's vocabulary\n",
    "# def count_unk(text, vocab):\n",
    "#     UNK_IDX = vocab.stoi['<UNK>']\n",
    "#     token_ids = vocab.numericalize(text)\n",
    "#     num_unk = token_ids.count(UNK_IDX)\n",
    "#     return num_unk\n",
    "\n",
    "# # count number of unknown tokens\n",
    "# demographic_train_df['num_unk'] = demographic_train_df.apply(\n",
    "#     lambda row : count_unk(row['text'], train_dataset.vocab),\n",
    "#     axis = 1\n",
    "# )\n",
    "# demographic_dev_df['num_unk'] = demographic_dev_df.apply(\n",
    "#     lambda row : count_unk(row['text'], train_dataset.vocab),\n",
    "#     axis = 1\n",
    "# )\n",
    "\n",
    "# # filter\n",
    "# demographic_train_df_filtered = demographic_train_df[demographic_train_df['num_unk'] <= 6]\n",
    "# demographic_dev_df_filtered = demographic_dev_df[demographic_dev_df['num_unk'] <= 6]\n",
    "\n",
    "# # resample each class to have the same number of examples\n",
    "# n_samples_per_class = 140000\n",
    "# demog_dfs = []\n",
    "# for demog in demographic_train_df_filtered['demographic'].unique():\n",
    "#     demog_df = demographic_train_df_filtered[demographic_train_df_filtered['demographic'] == demog]\n",
    "#     demog_df_sample = demog_df.sample(n=n_samples_per_class)\n",
    "#     demog_dfs.append(demog_df_sample)\n",
    "# demographic_train_df_resampled = pd.concat(demog_dfs)\n",
    "\n",
    "# n_samples_per_class = 48000\n",
    "# demog_dfs = []\n",
    "# for demog in demographic_dev_df_filtered['demographic'].unique():\n",
    "#     demog_df = demographic_dev_df_filtered[demographic_dev_df_filtered['demographic'] == demog]\n",
    "#     demog_df_sample = demog_df.sample(n=n_samples_per_class)\n",
    "#     demog_dfs.append(demog_df_sample)\n",
    "# demographic_dev_df_resampled = pd.concat(demog_dfs)\n",
    "\n",
    "# # save\n",
    "# demographic_train_df_resampled[['text', 'demographic']].sample(frac=1).to_csv(\n",
    "#     'civility_extra_data/demographic_train_balanced.tsv', \n",
    "#     sep = '\\t', \n",
    "#     index = False\n",
    "# )\n",
    "# demographic_dev_df_resampled[['text', 'demographic']].sample(frac=1).to_csv(\n",
    "#     'civility_extra_data/demographic_dev_balanced.tsv', \n",
    "#     sep = '\\t', \n",
    "#     index = False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "391db432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "demographic_train_df = pd.read_csv('civility_extra_data/demographic_train_balanced.tsv', sep='\\t')\n",
    "demographic_dev_df = pd.read_csv('civility_extra_data/demographic_dev_balanced.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3d541224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>demographic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>algebra hw is confusing me ._.</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yeah, im single</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@USER hola como estas</td>\n",
       "      <td>Hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>El Verano, El Verano Es Azul</td>\n",
       "      <td>Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Holla if ya need me, ya always gonna be my boo, holla if ya need me, you know i still got youu</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                             text  \\\n",
       "0                                                                  algebra hw is confusing me ._.   \n",
       "1                                                                                 Yeah, im single   \n",
       "2                                                                           @USER hola como estas   \n",
       "3                                                                    El Verano, El Verano Es Azul   \n",
       "4  Holla if ya need me, ya always gonna be my boo, holla if ya need me, you know i still got youu   \n",
       "\n",
       "  demographic  \n",
       "0    Hispanic  \n",
       "1    Hispanic  \n",
       "2    Hispanic  \n",
       "3       Other  \n",
       "4       White  "
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8e04e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "# but we need to overwrite the vocabulary with that of the main task\n",
    "demographic_train_dataset = TextClassifierTrainDataset(\n",
    "    df = demographic_train_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'demographic',\n",
    "    class_names = demographic_train_df['demographic'].unique().tolist(),\n",
    "    freq_threshold = 3,\n",
    "    max_size = None,\n",
    "    vocab = train_dataset.vocab\n",
    ")\n",
    "demographic_dev_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = demographic_train_dataset,\n",
    "    df = demographic_dev_df,\n",
    "    source_column = 'text', \n",
    "    target_column = 'demographic',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "556579a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "demographic_train_dataloader = get_train_loader(\n",
    "    dataset = demographic_train_dataset, \n",
    "    batch_size = 128\n",
    ")\n",
    "demographic_dev_dataloader   = get_inference_loader(\n",
    "    dataset = demographic_dev_dataset, \n",
    "    train_dataset = demographic_train_dataset,\n",
    "    batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4f8ad5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters (main classifier): 872194\n",
      "Number of model parameters (adversarial discriminator): 33412\n",
      "Number of model parameters (combined): 905606\n"
     ]
    }
   ],
   "source": [
    "# initialize all necessary models\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PAD_IDX = train_dataset.vocab.stoi['<PAD>']\n",
    "BOS_IDX = train_dataset.vocab.stoi['<SOS>']\n",
    "EOS_IDX = train_dataset.vocab.stoi['<EOS>']\n",
    "\n",
    "#==========================================================================================================================\n",
    "# initialize main classifier with pre-trained weights\n",
    "VOCAB_SIZE = len(train_dataset.vocab.stoi)\n",
    "EMB_SIZE = 64\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "mainClassifier = TransformerClassifier(\n",
    "    num_encoder_layers = NUM_ENCODER_LAYERS,\n",
    "    emb_size = EMB_SIZE,\n",
    "    nhead = NHEAD,\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    dim_feedforward = FFN_HID_DIM,\n",
    "    dropout = DROPOUT,\n",
    "    return_feature_vector = True\n",
    ")\n",
    "mainClassifier = mainClassifier.to(DEVICE)\n",
    "cls_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "cls_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "checkpoint = torch.load('checkpoints/transformer_v1_14_valacc0.7817220543806647.model', map_location=DEVICE)\n",
    "mainClassifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "cls_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "print('Number of model parameters (main classifier):', count_parameters(mainClassifier))\n",
    "\n",
    "#==========================================================================================================================\n",
    "# initialize adversarial discriminator model\n",
    "VOCAB_SIZE = len(demographic_train_dataset.vocab.stoi)\n",
    "DISCRIMINATOR_IN_DIM = 256\n",
    "DISCRIMINATOR_HIDDEN_SIZE = 128\n",
    "DISCRIMINATOR_DROPOUT = 0.3\n",
    "\n",
    "# initialize model\n",
    "discriminator = DemographicDiscriminator(\n",
    "    input_dim = DISCRIMINATOR_IN_DIM, \n",
    "    hidden_size = DISCRIMINATOR_HIDDEN_SIZE,\n",
    "    num_classes = len(demographic_train_dataset.idx_to_class),\n",
    "    dropout = DISCRIMINATOR_DROPOUT\n",
    ")\n",
    "for p in discriminator.parameters():\n",
    "    if p.dim() > 1:\n",
    "        nn.init.xavier_uniform_(p)\n",
    "discriminator = discriminator.to(DEVICE)\n",
    "dsc_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "dsc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.003, betas=(0.9, 0.98), eps=1e-9)\n",
    "print('Number of model parameters (adversarial discriminator):', count_parameters(discriminator))\n",
    "\n",
    "#==========================================================================================================================\n",
    "# combine with offensiveness classifier as feature extractor\n",
    "combined_model = CombinedModel(\n",
    "    mainClassifier = mainClassifier, \n",
    "    adversarialDiscriminator = discriminator\n",
    ")\n",
    "print('Number of model parameters (combined):', count_parameters(combined_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad399768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator_epoch(combined_model, loss_fn, train_dataloader, optimizer, DEVICE):\n",
    "    '''\n",
    "    Trains the adversarial disciminator ONLY, keeping the main classifier's weights fixed\n",
    "    '''\n",
    "    for param in combined_model.mainClassifier.parameters():\n",
    "        param.requires_grad = False\n",
    "    combined_model.train()\n",
    "    losses = 0\n",
    "    for i, (src, tgt) in enumerate(train_dataloader):\n",
    "                \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        _, discriminatorLogits = combined_model(src)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(discriminatorLogits.reshape(-1, discriminatorLogits.shape[-1]), tgt.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(train_dataloader)}: loss {loss}', end='', flush=True)\n",
    "\n",
    "    return losses / len(train_dataloader)\n",
    "\n",
    "def evaluate_discriminator(combined_model, loss_fn, val_dataloader, idx_to_class, DEVICE):\n",
    "    '''\n",
    "    Evaluates the loss and accuracy of the discriminator's predictions\n",
    "    '''\n",
    "    combined_model.eval()\n",
    "    losses = 0\n",
    "\n",
    "    # str_pred = []\n",
    "    # str_true = []\n",
    "    idx_pred = []\n",
    "    idx_true = []\n",
    "    for i, (src, tgt) in enumerate(val_dataloader):\n",
    "                \n",
    "        src = src.to(DEVICE)\n",
    "        tgt = tgt.to(DEVICE)\n",
    "        _, discriminatorLogits = combined_model(src)\n",
    "\n",
    "        # predictions of discriminator\n",
    "        tgt_pred = torch.max(discriminatorLogits, dim=-1).indices  # (T, N)\n",
    "        idx_pred.extend(tgt_pred.tolist())\n",
    "        idx_true.extend(tgt.tolist())\n",
    "\n",
    "        # # loop over examples in batch\n",
    "        # for x in range(tgt_pred.shape[-1]):\n",
    "        #     tgt_x      = tgt[x]\n",
    "        #     tgt_pred_x = tgt_pred[x]\n",
    "        #     tgt_x_string      = idx_to_class[tgt_x.tolist()]\n",
    "        #     tgt_pred_x_string = idx_to_class[tgt_pred_x.tolist()]\n",
    "        #     str_true.append(tgt_x_string)\n",
    "        #     str_pred.append(tgt_pred_x_string)\n",
    "        \n",
    "        loss = loss_fn(discriminatorLogits.reshape(-1, discriminatorLogits.shape[-1]), tgt.reshape(-1))\n",
    "        losses += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(val_dataloader)}: loss {loss}', end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return losses / len(val_dataloader), np.mean([int(te == pe) for te, pe in zip(idx_true, idx_pred)]), idx_true, idx_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5d9d2291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1400 of 1500: loss 1.3362728357315063\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 1, Train loss: 1.246, Epoch time = 37.285s\n",
      "Epoch: 1, Val loss  : 1.374, Val acc  : 0.378,   Val F1  : 0.295 Epoch time = 37.285s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.3377085924148568\n",
      "\n",
      "Epoch: 2, Train loss: 1.235, Epoch time = 37.938s\n",
      "Epoch: 2, Val loss  : 1.363, Val acc  : 0.365,   Val F1  : 0.281 Epoch time = 37.938s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.2962299585342407\n",
      "\n",
      "Epoch: 3, Train loss: 1.234, Epoch time = 37.183s\n",
      "Epoch: 3, Val loss  : 1.329, Val acc  : 0.375,   Val F1  : 0.292 Epoch time = 37.183s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.3028684854507446\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 4, Train loss: 1.233, Epoch time = 37.361s\n",
      "Epoch: 4, Val loss  : 1.341, Val acc  : 0.380,   Val F1  : 0.298 Epoch time = 37.361s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.2552243471145634\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 5, Train loss: 1.233, Epoch time = 40.161s\n",
      "Epoch: 5, Val loss  : 1.282, Val acc  : 0.388,   Val F1  : 0.308 Epoch time = 40.161s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.2634682655334473\n",
      "\n",
      "Epoch: 6, Train loss: 1.233, Epoch time = 38.425s\n",
      "Epoch: 6, Val loss  : 1.303, Val acc  : 0.377,   Val F1  : 0.298 Epoch time = 38.425s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.2704312801361084\n",
      "\n",
      "Epoch: 7, Train loss: 1.233, Epoch time = 38.602s\n",
      "Epoch: 7, Val loss  : 1.303, Val acc  : 0.382,   Val F1  : 0.300 Epoch time = 38.602s\n",
      "================================================================================================================\n",
      "Processing batch 1400 of 1500: loss 1.2820131778717043\n",
      "\n",
      "Epoch: 8, Train loss: 1.233, Epoch time = 38.946s\n",
      "Epoch: 8, Val loss  : 1.334, Val acc  : 0.379,   Val F1  : 0.296 Epoch time = 38.946s\n",
      "================================================================================================================\n",
      "Processing batch 3000 of 4375: loss 1.3225455284118652"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10160/1256118374.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# train one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     train_loss = train_discriminator_epoch(\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcombined_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mloss_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdsc_loss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10160/1018579582.py\u001b[0m in \u001b[0;36mtrain_discriminator_epoch\u001b[1;34m(combined_model, loss_fn, train_dataloader, optimizer, DEVICE)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminatorLogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcombined_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\masters\\spring-1\\11-830-computational-ethics-for-nlp\\homeworks\\HW2\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mmainClassifierLogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmainClassifierFeatureVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m         \u001b[0madversarialDiscriminatorLogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madversarialDiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmainClassifierFeatureVec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmainClassifierLogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madversarialDiscriminatorLogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\masters\\spring-1\\11-830-computational-ethics-for-nlp\\homeworks\\HW2\\model.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[0msrc_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msrc_tok_emb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0msrc_emb_enc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpositional_encoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0mencoder_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc_emb_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mencoder_agg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# take mean over sequence length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[0;32m    342\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         return F.layer_norm(\n\u001b[0m\u001b[0;32m    190\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\comp_ethics\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2484\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2485\u001b[0m         )\n\u001b[1;32m-> 2486\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2487\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train the adversarial discriminator ONLY; keeping the main classifier's weights fixed\n",
    "model = combined_model\n",
    "NUM_EPOCHS = 50\n",
    "PATIENCE = 5\n",
    "best_val_f1 = -1\n",
    "best_model_state_dict = None\n",
    "epochs_since_last_improved = 0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "\n",
    "    # grab last best model if not improved for `patience` epochs\n",
    "    if epochs_since_last_improved >= PATIENCE:\n",
    "        print(f'{epochs_since_last_improved} epochs since last improved. Loading from checkpoint...')\n",
    "        model.adversarialDiscriminator.load_state_dict(best_model_state_dict['model_state_dict'])\n",
    "        optimizer.load_state_dict(best_model_state_dict['optimizer_state_dict'])\n",
    "        epochs_since_last_improved = 0\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    # train one epoch\n",
    "    train_loss = train_discriminator_epoch(\n",
    "        combined_model = model, \n",
    "        loss_fn = dsc_loss_fn, \n",
    "        train_dataloader = demographic_train_dataloader, \n",
    "        optimizer = dsc_optimizer,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "\n",
    "    # evaluate train and val set at end of epoch\n",
    "    # train_loss, train_acc, train_true, train_pred = evaluate_discriminator(\n",
    "    #     combined_model = model, \n",
    "    #     loss_fn = dsc_loss_fn, \n",
    "    #     val_dataloader = demographic_train_dataloader,\n",
    "    #     idx_to_class = demographic_train_dataset.idx_to_class,\n",
    "    #     DEVICE = DEVICE\n",
    "    # )\n",
    "    val_loss, val_acc, val_true, val_pred = evaluate_discriminator(\n",
    "        combined_model = model, \n",
    "        loss_fn = dsc_loss_fn, \n",
    "        val_dataloader = demographic_dev_dataloader,\n",
    "        idx_to_class = demographic_train_dataset.idx_to_class,\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "    val_f1 = f1_score(val_true, val_pred, average='macro')  # penalizes bad performance on minority classs\n",
    "    \n",
    "    end_time = timer()\n",
    "\n",
    "    # checkpoint if train acc is better than last\n",
    "    if val_f1 > best_val_f1:\n",
    "        print(f'New best model found! saving...')\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.adversarialDiscriminator.state_dict(),\n",
    "            'optimizer_state_dict': dsc_optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'val_f1': val_f1\n",
    "        }\n",
    "        torch.save(state_dict, f'checkpoints/discriminator_v1_{epoch}_valf1{val_f1}.model')\n",
    "        best_val_f1 = val_f1\n",
    "        best_model_state_dict = state_dict\n",
    "        epochs_since_last_improved = 0\n",
    "    else:\n",
    "        epochs_since_last_improved += 1\n",
    "\n",
    "    print()\n",
    "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print((f\"Epoch: {epoch}, Val loss  : {val_loss:.3f}, Val acc  : {val_acc:.3f},   Val F1  : {val_f1:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "           \n",
    "    print(\"================================================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "e8bfa524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedModel(\n",
       "  (mainClassifier): TransformerClassifier(\n",
       "    (src_tok_emb): TokenEmbedding(\n",
       "      (embedding): Embedding(4574, 64)\n",
       "    )\n",
       "    (positional_encoding): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (transformer_encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=64, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=64, bias=True)\n",
       "          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fc1): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (fc1_activation): Tanh()\n",
       "    (fc1_dropout): Dropout(p=0.5, inplace=False)\n",
       "    (classifier): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       "  (adversarialDiscriminator): DemographicDiscriminator(\n",
       "    (fc1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (fc1_activation): Tanh()\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "    (classifier): Linear(in_features=128, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model\n",
    "checkpoint = torch.load('checkpoints/discriminator_v1_5_valf10.34552287643483504.model', map_location=DEVICE)\n",
    "model.adversarialDiscriminator.load_state_dict(checkpoint['model_state_dict'])\n",
    "dsc_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e835e722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42036979166666666"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['val_acc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5ebd92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 1400 of 1500: loss 1.1966357231140137\n",
      "0.34552287643483504\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc, val_true, val_pred = evaluate_discriminator(\n",
    "    combined_model = model, \n",
    "    loss_fn = dsc_loss_fn, \n",
    "    val_dataloader = demographic_dev_dataloader,\n",
    "    idx_to_class = demographic_train_dataset.idx_to_class,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "val_f1 = f1_score(val_true, val_pred, average='macro')  # penalizes bad performance on minority classs\n",
    "print(val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d8411cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1493, 23045, 16508,  6954],\n",
       "       [   29, 45981,  1819,   171],\n",
       "       [ 1772, 18333, 22784,  5111],\n",
       "       [ 1449, 26440,  9658, 10453]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "val_true_str = [demographic_train_dataset.idx_to_class[x] for x in val_true]\n",
    "val_pred_str = [demographic_train_dataset.idx_to_class[x] for x in val_pred]\n",
    "confusion_matrix(val_true_str, val_pred_str, labels=list(demographic_train_dataset.idx_to_class.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aa2f4bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Hispanic', 1: 'Other', 2: 'White', 3: 'AA'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic_train_dataset.idx_to_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9104180",
   "metadata": {},
   "source": [
    "Not very good, but it's not the discriminator's fault; it has limited control over the features that the main classifier (which serves as feature extractor) learns. This just tells us that the main classifier is not that biased to begin with (which is good)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2031383",
   "metadata": {},
   "source": [
    "# Train adversarially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3a119b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_adversarial(\n",
    "    combined_model, \n",
    "    cls_loss_fn, \n",
    "    dsc_loss_fn, \n",
    "    train_dataloader, \n",
    "    combined_optimizer,\n",
    "    DEVICE\n",
    "):\n",
    "    \n",
    "    '''\n",
    "    Train the main classifier with the adversarial objective, but freeze the adversarial discriminator's weights\n",
    "    '''\n",
    "    \n",
    "    # unfreeze main classifier weights\n",
    "    # for param in combined_model.mainClassifier.parameters():\n",
    "    #     param.requires_grad = True\n",
    "    \n",
    "    # freeze discriminator weights\n",
    "    for param in combined_model.adversarialDiscriminator.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # generate a uniform distribution over demographic classes as target for adversarial discriminator\n",
    "    uniform_tensor = torch.ones([128, 4]) / 4\n",
    "    uniform_tensor = uniform_tensor.to(DEVICE)\n",
    "    \n",
    "    # combined_model.to(DEVICE)\n",
    "    combined_model.train()\n",
    "    cls_losses = 0\n",
    "    dsc_losses = 0\n",
    "    total_losses = 0\n",
    "    for i, (src, cls_tgt) in enumerate(train_dataloader):\n",
    "                \n",
    "        # forward pass, obtaining output logits of both heads\n",
    "        src = src.to(DEVICE)\n",
    "        cls_tgt = cls_tgt.to(DEVICE)\n",
    "        dsc_tgt = uniform_tensor[:len(cls_tgt)]\n",
    "        mainClassifierLogits, discriminatorLogits = combined_model(src)\n",
    "        \n",
    "        # calculate gradients based on both losses\n",
    "        dsc_loss_weight = 1\n",
    "        cls_optimizer.zero_grad()\n",
    "        dsc_optimizer.zero_grad()\n",
    "        cls_loss = cls_loss_fn(mainClassifierLogits.reshape(-1, mainClassifierLogits.shape[-1]), cls_tgt.reshape(-1))\n",
    "        dsc_loss = dsc_loss_fn(discriminatorLogits, dsc_tgt)\n",
    "        total_loss = cls_loss + dsc_loss_weight * dsc_loss\n",
    "        # total_loss = dsc_loss\n",
    "        total_loss.backward()\n",
    "\n",
    "        # update weights based on combined loss\n",
    "        combined_optimizer.step()\n",
    "        \n",
    "        cls_losses += cls_loss.item()\n",
    "        dsc_losses += dsc_loss.item()\n",
    "        total_losses += total_loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(train_dataloader)}: cls_loss {cls_loss:.4f}, dsc_loss {dsc_loss:.4f}, total loss {total_loss:.4f}', end='', flush=True)\n",
    "    \n",
    "    print()\n",
    "    return total_losses / len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate_adversarial(\n",
    "    combined_model, \n",
    "    cls_loss_fn, \n",
    "    dsc_loss_fn, \n",
    "    val_dataloader, \n",
    "    DEVICE\n",
    "):\n",
    "    '''\n",
    "    Evaluates the loss and accuracy of the discriminator's predictions\n",
    "    '''\n",
    "    # combined_model.to(DEVICE)\n",
    "    combined_model.eval()\n",
    "    \n",
    "    # tracking for main classification task\n",
    "    cls_losses = 0\n",
    "    cls_idx_pred = []\n",
    "    cls_idx_true = []\n",
    "    \n",
    "    # tracking for adversarial task\n",
    "    dsc_losses = 0\n",
    "    dsc_idx_logits = []\n",
    "    dsc_idx_pred = []\n",
    "    \n",
    "    # generate a uniform distribution over demographic classes as target for adversarial discriminator\n",
    "    uniform_tensor = torch.ones([128, 4]) / 4\n",
    "    uniform_tensor = uniform_tensor.to(DEVICE)\n",
    "    \n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    total_losses = 0\n",
    "    for i, (src, cls_tgt) in enumerate(val_dataloader):\n",
    "                \n",
    "        # forward pass, obtaining output logits of both heads\n",
    "        src = src.to(DEVICE)\n",
    "        cls_tgt = cls_tgt.to(DEVICE)\n",
    "        dsc_tgt = uniform_tensor[:len(cls_tgt)]\n",
    "        mainClassifierLogits, discriminatorLogits = combined_model(src)\n",
    "\n",
    "        # predictions of main classifier\n",
    "        cls_tgt_pred = torch.max(mainClassifierLogits, dim=-1).indices  # (T, N)\n",
    "        cls_idx_pred.extend(cls_tgt_pred.cpu().detach().numpy().tolist())\n",
    "        cls_idx_true.extend(cls_tgt.cpu().detach().numpy().tolist())\n",
    "        \n",
    "        # predictions of discriminator, in probability distribution form\n",
    "        dsc_idx_probs = softmax(discriminatorLogits)\n",
    "        dsc_idx_pred.append(dsc_idx_probs.cpu().detach().numpy())\n",
    "        dsc_idx_logits.append(discriminatorLogits.cpu().detach().numpy())\n",
    "        \n",
    "        # calculate losses\n",
    "        dsc_loss_weight = 1\n",
    "        cls_loss = cls_loss_fn(mainClassifierLogits.reshape(-1, mainClassifierLogits.shape[-1]), cls_tgt.reshape(-1))\n",
    "        dsc_loss = dsc_loss_fn(discriminatorLogits, dsc_tgt)\n",
    "        total_loss = cls_loss + dsc_loss_weight * dsc_loss\n",
    "        \n",
    "        cls_losses += cls_loss.item()\n",
    "        dsc_losses += dsc_loss.item()\n",
    "        total_losses += total_loss.item()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            print(f'\\rProcessing batch {i} of {len(val_dataloader)}: loss {total_loss}', end='', flush=True)\n",
    "\n",
    "    print()\n",
    "    return \\\n",
    "        total_losses / len(val_dataloader), \\\n",
    "        cls_losses   / len(val_dataloader), \\\n",
    "        dsc_losses   / len(val_dataloader), \\\n",
    "        np.mean([int(te == pe) for te, pe in zip(cls_idx_true, cls_idx_pred)]), \\\n",
    "        cls_idx_true, \\\n",
    "        cls_idx_pred, \\\n",
    "        np.concatenate(dsc_idx_logits), \\\n",
    "        np.concatenate(dsc_idx_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42667d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10592, 3)\n",
      "(24783, 7)\n",
      "(1324, 4)\n",
      "(5072, 4)\n"
     ]
    }
   ],
   "source": [
    "# train data \n",
    "train_df = pd.read_csv('civility_data/train.tsv', sep='\\t')\n",
    "print(train_df.shape)\n",
    "\n",
    "# extra_data\n",
    "extra_df = pd.read_csv('civility_extra_data/labeled_data.tsv', sep='\\t')\n",
    "print(extra_df.shape)\n",
    "\n",
    "# dev data\n",
    "dev_df = pd.read_csv('civility_data/dev.tsv', sep='\\t')\n",
    "print(dev_df.shape)\n",
    "\n",
    "# demog dev: for FPR disparity\n",
    "demog_dev_df = pd.read_csv('civility_data/mini_demographic_dev.tsv', sep='\\t')\n",
    "demog_dev_df['label'] = 'NOT'\n",
    "print(demog_dev_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d2754ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets\n",
    "train_dataset = TextClassifierTrainDataset(\n",
    "    df = train_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    "    class_names = train_df['label'].unique().tolist(),\n",
    "    freq_threshold = 3,\n",
    "    max_size = None\n",
    ")\n",
    "val_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = dev_df, # val_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    ")\n",
    "demog_dev_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = demog_dev_df, \n",
    "    source_column = 'text', \n",
    "    target_column = 'label',\n",
    "    protected_attribute_column = 'demographic',\n",
    "    protected_attribute_names = demog_dev_df['demographic'].unique().tolist(),\n",
    "    protected_attribute_source = 'self'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a15b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_dataloader = get_train_loader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = 32\n",
    ")\n",
    "train_eval_dataloader = get_inference_loader(\n",
    "    dataset = train_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32\n",
    ")\n",
    "val_dataloader   = get_inference_loader(\n",
    "    dataset = val_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32\n",
    ")\n",
    "demog_dev_dataloader = get_inference_loader(\n",
    "    dataset = demog_dev_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32,\n",
    "    return_protected_attribute = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f99357f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters (main classifier): 872194\n",
      "Number of model parameters (adversarial discriminator): 33412\n",
      "Number of model parameters (combined): 905606\n"
     ]
    }
   ],
   "source": [
    "# initialize all necessary models\n",
    "\n",
    "torch.manual_seed(0)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "PAD_IDX = train_dataset.vocab.stoi['<PAD>']\n",
    "BOS_IDX = train_dataset.vocab.stoi['<SOS>']\n",
    "EOS_IDX = train_dataset.vocab.stoi['<EOS>']\n",
    "\n",
    "#==========================================================================================================================\n",
    "# initialize main classifier with pre-trained weights\n",
    "VOCAB_SIZE = len(train_dataset.vocab.stoi)\n",
    "EMB_SIZE = 64\n",
    "NHEAD = 4\n",
    "FFN_HID_DIM = 256\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "mainClassifier = TransformerClassifier(\n",
    "    num_encoder_layers = NUM_ENCODER_LAYERS,\n",
    "    emb_size = EMB_SIZE,\n",
    "    nhead = NHEAD,\n",
    "    vocab_size = VOCAB_SIZE,\n",
    "    dim_feedforward = FFN_HID_DIM,\n",
    "    dropout = DROPOUT,\n",
    "    return_feature_vector = True\n",
    ")\n",
    "mainClassifier = mainClassifier.to(DEVICE)\n",
    "cls_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "cls_optimizer = torch.optim.Adam(mainClassifier.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "print('Number of model parameters (main classifier):', count_parameters(mainClassifier))\n",
    "\n",
    "#==========================================================================================================================\n",
    "# initialize adversarial discriminator model\n",
    "VOCAB_SIZE = len(train_dataset.vocab.stoi)\n",
    "DISCRIMINATOR_IN_DIM = 256\n",
    "DISCRIMINATOR_HIDDEN_SIZE = 128\n",
    "DISCRIMINATOR_DROPOUT = 0.3\n",
    "\n",
    "# initialize model\n",
    "discriminator = DemographicDiscriminator(\n",
    "    input_dim = DISCRIMINATOR_IN_DIM, \n",
    "    hidden_size = DISCRIMINATOR_HIDDEN_SIZE,\n",
    "    num_classes = 4,  # white, AA, hispanic, other\n",
    "    dropout = DISCRIMINATOR_DROPOUT\n",
    ")\n",
    "discriminator = discriminator.to(DEVICE)\n",
    "dsc_loss_fn = torch.nn.CrossEntropyLoss()\n",
    "dsc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.003, betas=(0.9, 0.98), eps=1e-9)\n",
    "print('Number of model parameters (adversarial discriminator):', count_parameters(discriminator))\n",
    "\n",
    "#==========================================================================================================================\n",
    "# combine with offensiveness classifier as feature extractor\n",
    "combined_model = CombinedModel(\n",
    "    mainClassifier = mainClassifier, \n",
    "    adversarialDiscriminator = discriminator\n",
    ")\n",
    "\n",
    "# load main classifier weights\n",
    "cls_checkpoint = torch.load('checkpoints/transformer_v1_14_valacc0.7817220543806647.model', map_location=DEVICE)\n",
    "combined_model.mainClassifier.load_state_dict(cls_checkpoint['model_state_dict'])\n",
    "\n",
    "# load discriminator weights\n",
    "dsc_checkpoint = torch.load('checkpoints/discriminator_v1_5_valf10.34552287643483504.model', map_location=DEVICE)\n",
    "combined_model.adversarialDiscriminator.load_state_dict(dsc_checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer = torch.optim.Adam(combined_model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "combined_model.to(DEVICE)\n",
    "print('Number of model parameters (combined):', count_parameters(combined_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "090aec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 40 of 42: loss 2.4127669334411626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.8222176205544245"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check: before adversarial training,\n",
    "# discriminator's outputs should indicate that demographic is clearly predictable from the shared representation\n",
    "val_loss, val_cls_loss, val_dsc_loss, val_acc, val_true, val_pred, val_dsc_pred_logits, val_dsc_pred_probs = evaluate_adversarial(\n",
    "    combined_model = combined_model, \n",
    "    cls_loss_fn = cls_loss_fn, \n",
    "    dsc_loss_fn = dsc_loss_fn, \n",
    "    val_dataloader = val_dataloader,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "val_f1 = f1_score(val_true, val_pred)\n",
    "val_dsc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "798a16cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 190 of 200: loss 1.7995187044143677\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('civility_data/test.tsv', sep='\\t')\n",
    "test_df['dummy_label'] = 'NOT'\n",
    "\n",
    "test_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = test_df,\n",
    "    source_column = 'text', \n",
    "    target_column = 'dummy_label',\n",
    ")\n",
    "\n",
    "test_dataloader   = get_inference_loader(\n",
    "    dataset = test_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val_loss, val_cls_loss, val_dsc_loss, val_acc, val_true, val_pred, val_dsc_pred_logits, val_dsc_pred_probs = evaluate_adversarial(\n",
    "    combined_model = combined_model, \n",
    "    cls_loss_fn = cls_loss_fn, \n",
    "    dsc_loss_fn = dsc_loss_fn, \n",
    "    val_dataloader = test_dataloader,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "\n",
    "val_pred_str = [train_dataset.idx_to_class[x] for x in val_pred]\n",
    "\n",
    "test_df['label'] = val_pred_str\n",
    "\n",
    "test_df[['text', 'label']].to_csv('IgnatiusHarisSetiawan_Widjaja_test.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fadaeb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/1106396005.py:26: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from math import log2\n",
    "\n",
    "# custom crossentropy\n",
    "def cross_entropy(p, q):\n",
    "    return -sum([p[i]*log2(q[i]) for i in range(len(p))])\n",
    "\n",
    "num_rows = 3\n",
    "num_cols = 10\n",
    "pick_ids = np.random.choice(np.arange(len(val_dsc_pred_probs)), size=num_rows*num_cols, replace=False)\n",
    "demogs = ['Hispanic', 'Other', 'White', 'AA']\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(16,5))\n",
    "for i in range(num_rows*num_cols):\n",
    "    \n",
    "    # determine location in plot\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    \n",
    "    # grab probabilities and logits\n",
    "    example_id = pick_ids[i]\n",
    "    val_dsc_pred_probs_i = val_dsc_pred_probs[example_id:example_id+1]\n",
    "    val_dsc_pred_logits_i = val_dsc_pred_logits[example_id:example_id+1]\n",
    "    \n",
    "    # plot in the correct place\n",
    "    axs[row, col].bar(demogs, val_dsc_pred_probs_i.squeeze())\n",
    "    axs[row, col].set_xticks([])\n",
    "    axs[row, col].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in combined_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "713dfbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 330 of 331: cls_loss 0.3168, dsc_loss 1.4334, total loss 1.7502\n",
      "Processing batch 330 of 331: loss 1.7685060501098633\n",
      "Processing batch 40 of 42: loss 1.8574019670486458\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 1, Train cls loss: 0.433, dsc loss: 1.421 Epoch time = 5.044s\n",
      "Epoch: 1, Val cls loss  : 0.592, dsc loss: 1.420 Epoch time = 5.044s\n",
      "Epoch: 1, Val cls acc   : 0.780, F1: 0.850 Epoch time = 5.044s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3434, dsc_loss 1.4162, total loss 1.7596\n",
      "Processing batch 330 of 331: loss 1.7580800056457522\n",
      "Processing batch 40 of 42: loss 1.7813484668731696\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 2, Train cls loss: 0.459, dsc loss: 1.405 Epoch time = 5.113s\n",
      "Epoch: 2, Val cls loss  : 0.628, dsc loss: 1.403 Epoch time = 5.113s\n",
      "Epoch: 2, Val cls acc   : 0.779, F1: 0.851 Epoch time = 5.113s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3657, dsc_loss 1.4161, total loss 1.7818\n",
      "Processing batch 330 of 331: loss 1.7239302396774292\n",
      "Processing batch 40 of 42: loss 1.7995679378509521\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 3, Train cls loss: 0.426, dsc loss: 1.399 Epoch time = 5.577s\n",
      "Epoch: 3, Val cls loss  : 0.614, dsc loss: 1.398 Epoch time = 5.577s\n",
      "Epoch: 3, Val cls acc   : 0.782, F1: 0.849 Epoch time = 5.577s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2942, dsc_loss 1.4115, total loss 1.7058\n",
      "Processing batch 330 of 331: loss 1.7258504629135132\n",
      "Processing batch 40 of 42: loss 1.7775783538818366\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 4, Train cls loss: 0.460, dsc loss: 1.396 Epoch time = 5.299s\n",
      "Epoch: 4, Val cls loss  : 0.658, dsc loss: 1.395 Epoch time = 5.299s\n",
      "Epoch: 4, Val cls acc   : 0.782, F1: 0.852 Epoch time = 5.299s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2971, dsc_loss 1.4244, total loss 1.7215\n",
      "Processing batch 330 of 331: loss 1.7076787948608398\n",
      "Processing batch 40 of 42: loss 1.7551126480102545\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 5, Train cls loss: 0.446, dsc loss: 1.391 Epoch time = 5.657s\n",
      "Epoch: 5, Val cls loss  : 0.653, dsc loss: 1.392 Epoch time = 5.657s\n",
      "Epoch: 5, Val cls acc   : 0.779, F1: 0.850 Epoch time = 5.657s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2953, dsc_loss 1.4209, total loss 1.7162\n",
      "Processing batch 330 of 331: loss 1.7218440771102905\n",
      "Processing batch 40 of 42: loss 1.7733163833618164\n",
      "\n",
      "Epoch: 6, Train cls loss: 0.449, dsc loss: 1.400 Epoch time = 5.065s\n",
      "Epoch: 6, Val cls loss  : 0.661, dsc loss: 1.398 Epoch time = 5.065s\n",
      "Epoch: 6, Val cls acc   : 0.778, F1: 0.849 Epoch time = 5.065s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3230, dsc_loss 1.4178, total loss 1.7407\n",
      "Processing batch 330 of 331: loss 1.6927398443222046\n",
      "Processing batch 40 of 42: loss 1.7592782974243164\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 7, Train cls loss: 0.456, dsc loss: 1.389 Epoch time = 5.133s\n",
      "Epoch: 7, Val cls loss  : 0.681, dsc loss: 1.390 Epoch time = 5.133s\n",
      "Epoch: 7, Val cls acc   : 0.778, F1: 0.848 Epoch time = 5.133s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2971, dsc_loss 1.4272, total loss 1.7243\n",
      "Processing batch 330 of 331: loss 1.6944468021392822\n",
      "Processing batch 40 of 42: loss 1.7744922637939453\n",
      "\n",
      "Epoch: 8, Train cls loss: 0.440, dsc loss: 1.397 Epoch time = 4.924s\n",
      "Epoch: 8, Val cls loss  : 0.677, dsc loss: 1.396 Epoch time = 4.924s\n",
      "Epoch: 8, Val cls acc   : 0.775, F1: 0.845 Epoch time = 4.924s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3419, dsc_loss 1.4237, total loss 1.7655\n",
      "Processing batch 330 of 331: loss 1.6858477592468262\n",
      "Processing batch 40 of 42: loss 1.7820791006088257\n",
      "\n",
      "Epoch: 9, Train cls loss: 0.425, dsc loss: 1.391 Epoch time = 5.338s\n",
      "Epoch: 9, Val cls loss  : 0.670, dsc loss: 1.391 Epoch time = 5.338s\n",
      "Epoch: 9, Val cls acc   : 0.777, F1: 0.847 Epoch time = 5.338s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2998, dsc_loss 1.4123, total loss 1.7121\n",
      "Processing batch 330 of 331: loss 1.6878217458724976\n",
      "Processing batch 40 of 42: loss 1.7329584360122684\n",
      "\n",
      "Epoch: 10, Train cls loss: 0.444, dsc loss: 1.392 Epoch time = 5.152s\n",
      "Epoch: 10, Val cls loss  : 0.691, dsc loss: 1.392 Epoch time = 5.152s\n",
      "Epoch: 10, Val cls acc   : 0.774, F1: 0.846 Epoch time = 5.152s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2932, dsc_loss 1.4184, total loss 1.7116\n",
      "Processing batch 330 of 331: loss 1.6817922592163086\n",
      "Processing batch 40 of 42: loss 1.7662266492843628\n",
      "\n",
      "Epoch: 11, Train cls loss: 0.428, dsc loss: 1.393 Epoch time = 5.221s\n",
      "Epoch: 11, Val cls loss  : 0.691, dsc loss: 1.393 Epoch time = 5.221s\n",
      "Epoch: 11, Val cls acc   : 0.773, F1: 0.845 Epoch time = 5.221s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3670, dsc_loss 1.4135, total loss 1.7805\n",
      "Processing batch 330 of 331: loss 1.6986722946166992\n",
      "Processing batch 40 of 42: loss 1.7285113334655762\n",
      "\n",
      "Epoch: 12, Train cls loss: 0.440, dsc loss: 1.392 Epoch time = 5.390s\n",
      "Epoch: 12, Val cls loss  : 0.716, dsc loss: 1.392 Epoch time = 5.390s\n",
      "Epoch: 12, Val cls acc   : 0.769, F1: 0.842 Epoch time = 5.390s\n",
      "================================================================================================================\n",
      "5 epochs since last improved. Loading from checkpoint...\n",
      "Processing batch 330 of 331: cls_loss 0.3040, dsc_loss 1.4155, total loss 1.7195\n",
      "Processing batch 330 of 331: loss 1.6816763877868652\n",
      "Processing batch 40 of 42: loss 1.7671711444854736\n",
      "\n",
      "Epoch: 13, Train cls loss: 0.401, dsc loss: 1.397 Epoch time = 5.225s\n",
      "Epoch: 13, Val cls loss  : 0.666, dsc loss: 1.398 Epoch time = 5.225s\n",
      "Epoch: 13, Val cls acc   : 0.775, F1: 0.845 Epoch time = 5.225s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2683, dsc_loss 1.4138, total loss 1.6821\n",
      "Processing batch 330 of 331: loss 1.6557884216308594\n",
      "Processing batch 40 of 42: loss 1.7810175418853768\n",
      "New best model found! saving...\n",
      "\n",
      "Epoch: 14, Train cls loss: 0.372, dsc loss: 1.389 Epoch time = 5.520s\n",
      "Epoch: 14, Val cls loss  : 0.659, dsc loss: 1.389 Epoch time = 5.520s\n",
      "Epoch: 14, Val cls acc   : 0.764, F1: 0.836 Epoch time = 5.520s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2676, dsc_loss 1.4197, total loss 1.6873\n",
      "Processing batch 330 of 331: loss 1.6816987991333008\n",
      "Processing batch 40 of 42: loss 1.7481267452239993\n",
      "\n",
      "Epoch: 15, Train cls loss: 0.422, dsc loss: 1.396 Epoch time = 5.406s\n",
      "Epoch: 15, Val cls loss  : 0.710, dsc loss: 1.397 Epoch time = 5.406s\n",
      "Epoch: 15, Val cls acc   : 0.770, F1: 0.843 Epoch time = 5.406s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.3101, dsc_loss 1.4226, total loss 1.7327\n",
      "Processing batch 330 of 331: loss 1.6639214754104614\n",
      "Processing batch 40 of 42: loss 1.7892311811447144\n",
      "\n",
      "Epoch: 16, Train cls loss: 0.381, dsc loss: 1.393 Epoch time = 5.132s\n",
      "Epoch: 16, Val cls loss  : 0.684, dsc loss: 1.394 Epoch time = 5.132s\n",
      "Epoch: 16, Val cls acc   : 0.766, F1: 0.837 Epoch time = 5.132s\n",
      "================================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 330 of 331: cls_loss 0.2306, dsc_loss 1.4153, total loss 1.6459\n",
      "Processing batch 330 of 331: loss 1.6655876636505127\n",
      "Processing batch 40 of 42: loss 1.7429814338684082\n",
      "\n",
      "Epoch: 17, Train cls loss: 0.393, dsc loss: 1.391 Epoch time = 5.160s\n",
      "Epoch: 17, Val cls loss  : 0.703, dsc loss: 1.391 Epoch time = 5.160s\n",
      "Epoch: 17, Val cls acc   : 0.764, F1: 0.837 Epoch time = 5.160s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2690, dsc_loss 1.4153, total loss 1.6843\n",
      "Processing batch 330 of 331: loss 1.6587021350860596\n",
      "Processing batch 40 of 42: loss 1.7690533399581914\n",
      "\n",
      "Epoch: 18, Train cls loss: 0.388, dsc loss: 1.391 Epoch time = 5.233s\n",
      "Epoch: 18, Val cls loss  : 0.708, dsc loss: 1.391 Epoch time = 5.233s\n",
      "Epoch: 18, Val cls acc   : 0.764, F1: 0.836 Epoch time = 5.233s\n",
      "================================================================================================================\n",
      "Processing batch 330 of 331: cls_loss 0.2481, dsc_loss 1.4089, total loss 1.6570\n",
      "Processing batch 330 of 331: loss 1.6573427915573126\n",
      "Processing batch 40 of 42: loss 1.7935678958892822\n",
      "\n",
      "Epoch: 19, Train cls loss: 0.362, dsc loss: 1.390 Epoch time = 5.048s\n",
      "Epoch: 19, Val cls loss  : 0.701, dsc loss: 1.391 Epoch time = 5.048s\n",
      "Epoch: 19, Val cls acc   : 0.757, F1: 0.829 Epoch time = 5.048s\n",
      "================================================================================================================\n",
      "5 epochs since last improved. Loading from checkpoint...\n",
      "Processing batch 330 of 331: cls_loss 0.2923, dsc_loss 1.4165, total loss 1.7088\n",
      "Processing batch 330 of 331: loss 1.6549124717712402\n",
      "Processing batch 40 of 42: loss 1.7645375728607178\n",
      "\n",
      "Epoch: 20, Train cls loss: 0.369, dsc loss: 1.391 Epoch time = 5.101s\n",
      "Epoch: 20, Val cls loss  : 0.714, dsc loss: 1.392 Epoch time = 5.101s\n",
      "Epoch: 20, Val cls acc   : 0.757, F1: 0.831 Epoch time = 5.101s\n",
      "================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# train the main classifier ONLY, but with the adversarial objective; keeping the adversarial discriminator's weights fixed\n",
    "model = combined_model\n",
    "NUM_EPOCHS = 20\n",
    "PATIENCE = 5\n",
    "best_val_dsc_loss = 10000\n",
    "best_model_state_dict = None\n",
    "epochs_since_last_improved = 0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "\n",
    "    # grab last best model if not improved for `patience` epochs\n",
    "    if epochs_since_last_improved >= PATIENCE:\n",
    "        print(f'{epochs_since_last_improved} epochs since last improved. Loading from checkpoint...')\n",
    "        model.load_state_dict(best_model_state_dict['model_state_dict'])\n",
    "        optimizer.load_state_dict(best_model_state_dict['optimizer_state_dict'])\n",
    "        epochs_since_last_improved = 0\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    # train one epoch\n",
    "    train_loss = train_epoch_adversarial(\n",
    "        combined_model = model, \n",
    "        cls_loss_fn = cls_loss_fn, \n",
    "        dsc_loss_fn = dsc_loss_fn, \n",
    "        train_dataloader = train_dataloader, \n",
    "        combined_optimizer = optimizer,  # combined optimizer\n",
    "        DEVICE = DEVICE\n",
    "    )\n",
    "\n",
    "    # evaluate train and val set at end of epoch\n",
    "    train_loss, train_cls_loss, train_dsc_loss, train_acc, train_true, train_pred, train_dsc_pred_logits, train_dsc_pred_probs = evaluate_adversarial(\n",
    "        combined_model = combined_model, \n",
    "        cls_loss_fn = cls_loss_fn, \n",
    "        dsc_loss_fn = dsc_loss_fn, \n",
    "        val_dataloader = train_eval_dataloader,\n",
    "        DEVICE = DEVICE # torch.device('cpu')\n",
    "    )\n",
    "    val_loss, val_cls_loss, val_dsc_loss, val_acc, val_true, val_pred, val_dsc_pred_logits, val_dsc_pred_probs = evaluate_adversarial(\n",
    "        combined_model = combined_model, \n",
    "        cls_loss_fn = cls_loss_fn, \n",
    "        dsc_loss_fn = dsc_loss_fn, \n",
    "        val_dataloader = val_dataloader,\n",
    "        DEVICE = DEVICE # torch.device('cpu')\n",
    "    )\n",
    "    val_f1 = f1_score(val_true, val_pred)\n",
    "    \n",
    "    end_time = timer()\n",
    "\n",
    "    # checkpoint if train acc is better than last\n",
    "    if val_dsc_loss < best_val_dsc_loss:\n",
    "        print(f'New best model found! saving...')\n",
    "        state_dict = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_cls_loss': train_cls_loss,\n",
    "            'train_dsc_loss': train_dsc_loss,\n",
    "            'val_cls_loss': val_cls_loss,\n",
    "            'val_dsc_loss': val_dsc_loss,\n",
    "            'val_cls_acc': val_acc,\n",
    "            'val_cls_f1': val_f1\n",
    "        }\n",
    "        torch.save(state_dict, f'checkpoints/combined_v1_{epoch}_valdscloss{val_dsc_loss:.6f}_valf1{val_f1:.6f}.model')\n",
    "        best_val_dsc_loss = val_dsc_loss\n",
    "        best_model_state_dict = state_dict\n",
    "        epochs_since_last_improved = 0\n",
    "    else:\n",
    "        epochs_since_last_improved += 1\n",
    "\n",
    "    print()\n",
    "    print((f\"Epoch: {epoch}, Train cls loss: {train_cls_loss:.3f}, dsc loss: {train_dsc_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print((f\"Epoch: {epoch}, Val cls loss  : {val_cls_loss:.3f}, dsc loss: {val_dsc_loss:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    print((f\"Epoch: {epoch}, Val cls acc   : {val_acc:.3f}, F1: {val_f1:.3f} \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
    "    \n",
    "    print(\"================================================================================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f54a2f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mainClassifier.src_tok_emb.embedding.weight tensor([[ 0.0724, -0.0463,  0.0273,  ...,  0.0577, -0.0026, -0.0456],\n",
      "        [ 0.0668, -0.0175, -0.0228,  ...,  0.0215,  0.0075, -0.0584],\n",
      "        [ 0.0368, -0.0101,  0.0330,  ...,  0.0347,  0.0024, -0.0315],\n",
      "        ...,\n",
      "        [ 0.0462, -0.0182, -0.0246,  ...,  0.0348,  0.0181,  0.0350],\n",
      "        [ 0.0350,  0.0076,  0.0281,  ..., -0.0118, -0.0127, -0.0147],\n",
      "        [ 0.0443, -0.0459, -0.0471,  ...,  0.0217,  0.0472,  0.0619]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.self_attn.in_proj_weight tensor([[ 0.1580, -0.0686,  0.1058,  ...,  0.1266,  0.0599, -0.1456],\n",
      "        [ 0.0763, -0.1274,  0.0191,  ...,  0.0812,  0.1610, -0.0870],\n",
      "        [ 0.0551,  0.0456, -0.1268,  ..., -0.1498, -0.1106, -0.1280],\n",
      "        ...,\n",
      "        [-0.0047,  0.1439,  0.0337,  ...,  0.0061,  0.0232, -0.0387],\n",
      "        [ 0.1224, -0.0628, -0.1126,  ..., -0.0320,  0.1756, -0.0564],\n",
      "        [ 0.0357, -0.0573,  0.1219,  ..., -0.1272,  0.2129,  0.1340]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.self_attn.in_proj_bias tensor([ 6.4532e-03, -4.2102e-02, -1.2232e-02, -6.2364e-03, -1.0477e-02,\n",
      "        -1.0644e-02, -1.2529e-02,  2.7176e-02,  6.9773e-04, -4.3871e-02,\n",
      "         4.5029e-02,  1.2712e-02, -1.1840e-02,  9.4882e-03, -3.3242e-03,\n",
      "         2.4233e-02, -2.3398e-02, -1.7564e-02, -3.8294e-03, -5.8324e-03,\n",
      "         2.1959e-02,  1.6296e-02, -2.1376e-02, -2.5898e-02,  1.3677e-02,\n",
      "         2.0477e-02, -1.6900e-02, -8.1253e-03, -1.2844e-02,  2.2835e-02,\n",
      "         1.2051e-02,  3.2838e-02, -2.2738e-02,  1.0152e-02, -5.2206e-03,\n",
      "        -1.4859e-02,  8.5004e-03,  2.3773e-03, -4.4652e-04,  1.1505e-02,\n",
      "        -3.9513e-03, -1.4208e-02,  2.5024e-02, -3.4211e-02, -1.4048e-02,\n",
      "        -3.0061e-02, -3.3571e-03,  1.0878e-02, -4.5134e-03,  1.7685e-02,\n",
      "         4.1004e-03,  2.3276e-02,  2.6435e-02,  2.1182e-02,  3.2176e-02,\n",
      "        -2.4739e-02, -1.6186e-02, -2.0758e-02, -1.0417e-02, -3.1208e-02,\n",
      "        -1.5270e-02, -1.7373e-02,  4.7241e-02, -1.1895e-02,  4.5891e-03,\n",
      "         3.9014e-03,  4.5881e-03,  3.8086e-03,  6.9523e-03, -5.9441e-03,\n",
      "        -1.9803e-02, -1.9891e-02,  5.7899e-03,  1.3120e-02, -1.0348e-02,\n",
      "         6.4676e-03,  6.1823e-03, -2.0771e-03, -6.9548e-03, -2.1905e-04,\n",
      "        -1.8071e-02, -4.0055e-03, -1.2562e-02,  2.4771e-03, -1.3818e-03,\n",
      "         7.8498e-03,  3.3514e-04, -1.3621e-02, -6.0092e-03,  9.7361e-03,\n",
      "        -8.0375e-03, -3.7499e-03, -2.8002e-03, -4.6831e-03,  1.8125e-02,\n",
      "         4.9902e-03, -6.6655e-03,  1.7398e-02, -5.6172e-03, -4.2353e-03,\n",
      "        -4.5793e-03,  7.4899e-03, -1.1190e-02,  1.7667e-03, -2.2019e-03,\n",
      "        -1.1406e-02,  1.2645e-02, -1.1313e-02, -1.7627e-02, -7.2780e-03,\n",
      "         1.1524e-03,  2.8979e-04,  1.2815e-02, -5.1778e-03, -4.3861e-03,\n",
      "        -2.0845e-03, -1.3970e-02,  3.2078e-03,  1.0487e-03,  4.3280e-03,\n",
      "         1.4949e-02, -1.4647e-02, -1.0741e-02,  1.0241e-02,  1.1357e-03,\n",
      "         1.1311e-03, -8.0994e-03, -5.3195e-03, -7.1863e-05,  5.0603e-03,\n",
      "         1.0180e-02, -3.7184e-03, -1.3555e-02, -1.4920e-03,  8.2900e-03,\n",
      "         1.7266e-02,  1.5518e-02,  4.8946e-03, -1.1247e-02, -3.8280e-03,\n",
      "         3.8370e-03,  4.6453e-03, -7.8832e-03,  1.3969e-02,  5.7149e-03,\n",
      "         5.7840e-03,  7.9937e-03, -6.4117e-03,  4.8700e-03, -2.6127e-03,\n",
      "         1.0544e-02, -2.3499e-03, -2.6037e-03, -2.0030e-04,  5.7769e-03,\n",
      "        -3.4096e-03,  6.7019e-03,  1.3375e-03,  1.2871e-03, -6.2997e-03,\n",
      "         1.1076e-02,  3.1823e-03,  4.5590e-03,  3.7465e-03, -2.3946e-03,\n",
      "         2.0770e-03, -5.0251e-03, -4.5226e-03,  4.3309e-03, -1.9142e-02,\n",
      "         1.8973e-03, -3.1305e-03, -3.8564e-04, -7.8389e-03, -6.1160e-03,\n",
      "         1.0778e-02, -4.2987e-03,  6.7140e-03,  2.1678e-03,  1.4926e-03,\n",
      "         3.8667e-03,  1.5230e-02,  5.5581e-03, -5.3888e-04,  2.3861e-04,\n",
      "         4.6424e-03,  9.3472e-03,  1.5103e-03, -1.1831e-02,  1.1016e-02,\n",
      "        -5.0447e-03, -1.0873e-02], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.self_attn.out_proj.weight tensor([[ 0.1015, -0.1749, -0.1577,  ...,  0.0184,  0.0584,  0.0060],\n",
      "        [-0.1997,  0.1057, -0.2173,  ...,  0.1144, -0.1868,  0.0343],\n",
      "        [ 0.2134, -0.1967, -0.1679,  ...,  0.1021, -0.2345, -0.0123],\n",
      "        ...,\n",
      "        [-0.0617, -0.0458,  0.1617,  ...,  0.0165, -0.1202, -0.0569],\n",
      "        [ 0.0524, -0.0595,  0.0744,  ...,  0.1555,  0.1487, -0.1637],\n",
      "        [-0.2181,  0.1987,  0.1124,  ..., -0.1610, -0.0419, -0.0904]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.self_attn.out_proj.bias tensor([ 0.0050,  0.0057,  0.0063, -0.0050, -0.0076,  0.0100,  0.0010,  0.0009,\n",
      "         0.0094,  0.0065,  0.0049, -0.0026,  0.0001, -0.0057,  0.0004,  0.0032,\n",
      "        -0.0002,  0.0026, -0.0092,  0.0050, -0.0007,  0.0146, -0.0047,  0.0071,\n",
      "        -0.0064,  0.0119,  0.0064, -0.0027,  0.0069, -0.0108,  0.0120, -0.0056,\n",
      "        -0.0048, -0.0117,  0.0036, -0.0072, -0.0017, -0.0054, -0.0067,  0.0048,\n",
      "         0.0020, -0.0002,  0.0115, -0.0042,  0.0069, -0.0041, -0.0048, -0.0111,\n",
      "         0.0065, -0.0083,  0.0170, -0.0041, -0.0071, -0.0143, -0.0012, -0.0187,\n",
      "        -0.0047, -0.0138,  0.0060,  0.0030,  0.0040, -0.0080,  0.0014, -0.0079],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.linear1.weight tensor([[-0.0235,  0.0175, -0.0090,  ..., -0.0176,  0.0502, -0.0478],\n",
      "        [ 0.0335,  0.0623, -0.0232,  ..., -0.0030,  0.0287, -0.0062],\n",
      "        [ 0.0193,  0.0027,  0.0212,  ..., -0.0211,  0.0780, -0.0116],\n",
      "        ...,\n",
      "        [-0.0353, -0.0328,  0.0176,  ...,  0.0330,  0.0048,  0.0101],\n",
      "        [-0.0367,  0.0217,  0.0594,  ...,  0.0271, -0.0259, -0.0251],\n",
      "        [ 0.0152,  0.0095,  0.0274,  ...,  0.0224,  0.0428, -0.0252]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.linear1.bias tensor([-0.0259,  0.0643, -0.1080,  ..., -0.0400,  0.0916, -0.0448],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.linear2.weight tensor([[-0.0004,  0.0544, -0.0256,  ...,  0.0423, -0.0151, -0.0479],\n",
      "        [-0.0410, -0.0076, -0.0232,  ..., -0.0216,  0.0053,  0.0250],\n",
      "        [ 0.0053,  0.0055, -0.0464,  ...,  0.0174, -0.0216,  0.0117],\n",
      "        ...,\n",
      "        [ 0.0184,  0.0047,  0.0356,  ...,  0.0287,  0.0105, -0.0189],\n",
      "        [ 0.0471, -0.0180, -0.0144,  ..., -0.0373,  0.0054, -0.0237],\n",
      "        [ 0.0448, -0.0266,  0.0445,  ...,  0.0083,  0.0423,  0.0319]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.linear2.bias tensor([ 8.7705e-03, -1.6227e-02,  6.9257e-03, -1.1124e-02,  4.0689e-03,\n",
      "        -1.4884e-02,  1.5653e-02,  2.1261e-02,  2.2626e-03,  6.8429e-03,\n",
      "         1.8184e-02,  1.0034e-02, -1.1022e-02, -2.2173e-02, -7.5121e-03,\n",
      "         9.8046e-04,  1.6005e-02, -8.8136e-03, -1.4269e-02,  4.9888e-03,\n",
      "        -2.6856e-03, -1.8875e-02, -1.7869e-02, -1.7045e-02, -1.8257e-02,\n",
      "        -8.2255e-03,  1.7587e-02,  1.9966e-02, -6.4606e-04, -1.3036e-02,\n",
      "        -1.9904e-03, -2.0484e-02,  1.1978e-02, -4.8331e-03,  5.9536e-03,\n",
      "         1.0431e-03,  1.0222e-02,  2.0777e-02,  2.3356e-03, -9.9632e-03,\n",
      "         1.0756e-02, -9.6497e-03, -5.8663e-03,  5.4143e-03, -5.7230e-03,\n",
      "        -4.8180e-03,  1.5510e-02, -2.0248e-02,  1.5982e-02, -2.2560e-02,\n",
      "        -2.7094e-03,  1.9871e-02,  1.0563e-02, -2.8541e-03,  1.5063e-02,\n",
      "         4.1959e-05, -1.4348e-02,  8.8206e-03,  1.5127e-02,  2.4808e-02,\n",
      "         1.6774e-02, -1.1182e-02, -2.3257e-03,  8.7190e-03], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.norm1.weight tensor([0.9876, 0.9959, 0.9977, 1.0005, 0.9905, 0.9754, 0.9741, 0.9973, 1.0041,\n",
      "        0.9905, 1.0028, 0.9979, 0.9837, 0.9995, 1.0107, 1.0156, 1.0035, 1.0334,\n",
      "        0.9980, 1.0048, 0.9932, 0.9839, 1.0004, 0.9775, 0.9738, 0.9840, 1.0068,\n",
      "        0.9460, 1.0227, 0.9870, 0.9930, 0.9912, 1.0216, 0.9906, 1.0282, 1.0075,\n",
      "        1.0115, 0.9913, 1.0162, 0.9788, 1.0286, 0.9699, 0.9879, 1.0075, 1.0022,\n",
      "        0.9916, 1.0422, 0.9823, 1.0035, 1.0019, 1.0011, 1.0089, 1.0124, 0.9837,\n",
      "        1.0140, 0.9779, 1.0509, 0.9768, 1.0272, 1.0024, 1.0047, 0.9903, 1.0046,\n",
      "        0.9841], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.norm1.bias tensor([ 4.6997e-03,  6.4042e-03,  6.7631e-03, -4.5928e-03, -7.6694e-03,\n",
      "         9.7114e-03,  9.7183e-05,  1.3385e-03,  9.3894e-03,  6.4397e-03,\n",
      "         4.4500e-03, -2.0673e-03, -1.8147e-03, -4.6419e-03,  1.5015e-03,\n",
      "         4.0117e-03, -8.6432e-04,  2.6206e-03, -7.8751e-03,  6.4471e-03,\n",
      "         6.2050e-04,  1.3148e-02, -2.6239e-03,  7.0716e-03, -5.6432e-03,\n",
      "         1.3552e-02,  6.2633e-03, -2.7743e-03,  5.6028e-03, -9.7963e-03,\n",
      "         1.1165e-02, -5.5649e-03, -4.2894e-03, -9.0851e-03,  3.7687e-03,\n",
      "        -6.8598e-03, -2.6322e-03, -4.9518e-03, -5.2763e-03,  3.6561e-03,\n",
      "         2.0390e-03, -4.9299e-04,  9.9777e-03, -3.6103e-03,  6.5413e-03,\n",
      "        -4.6165e-03, -3.1893e-03, -1.0167e-02,  5.4514e-03, -6.8519e-03,\n",
      "         1.9593e-02, -4.8249e-03, -5.3833e-03, -1.4011e-02, -1.1034e-03,\n",
      "        -1.8997e-02, -3.2999e-03, -1.2474e-02,  5.8797e-03,  2.6026e-03,\n",
      "         3.8232e-03, -7.7825e-03,  4.5784e-04, -7.4349e-03], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.norm2.weight tensor([0.9965, 1.0136, 1.0128, 1.0081, 0.9987, 0.9827, 0.9969, 1.0188, 1.0362,\n",
      "        1.0040, 1.0083, 0.9975, 1.0102, 1.0011, 1.0292, 1.0181, 1.0201, 1.0502,\n",
      "        1.0234, 1.0066, 1.0004, 1.0161, 1.0199, 0.9738, 0.9813, 0.9997, 1.0066,\n",
      "        0.9663, 1.0088, 0.9876, 0.9922, 0.9935, 0.9845, 1.0121, 1.0013, 1.0381,\n",
      "        0.9999, 0.9914, 1.0099, 0.9844, 0.9823, 0.9886, 0.9818, 1.0525, 1.0013,\n",
      "        1.0024, 1.0112, 0.9734, 0.9941, 1.0075, 0.9921, 1.0156, 0.9977, 1.0093,\n",
      "        0.9864, 1.0221, 1.0161, 0.9831, 1.0374, 1.0092, 0.9963, 1.0042, 1.0001,\n",
      "        0.9808], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.0.norm2.bias tensor([ 3.5168e-04,  1.3907e-03,  3.2886e-03, -4.3050e-03, -3.8643e-03,\n",
      "         2.0333e-03, -1.0956e-05, -1.4271e-04,  5.8924e-03, -9.2864e-04,\n",
      "         8.0294e-04,  2.8231e-04,  3.2110e-03, -3.7206e-03,  8.1419e-04,\n",
      "         1.9614e-03, -4.0114e-03,  3.5762e-03, -1.5595e-03,  2.6672e-03,\n",
      "         6.2868e-04,  4.3724e-04, -2.9456e-03,  4.4795e-03,  2.0841e-03,\n",
      "         6.3666e-03,  1.7217e-03,  8.8404e-04,  4.4006e-03, -3.5317e-03,\n",
      "         3.1903e-03, -6.1841e-04, -2.6802e-03,  5.0847e-03,  2.2815e-03,\n",
      "        -4.5389e-03, -1.0265e-03,  2.0503e-03, -3.3469e-03, -1.0835e-03,\n",
      "         1.4242e-03,  1.5778e-03,  2.9813e-03, -2.4802e-03,  3.2451e-03,\n",
      "        -1.1703e-03, -2.0536e-03, -4.7015e-03,  1.8152e-03, -8.8100e-04,\n",
      "         3.0676e-03, -1.3365e-03,  2.5857e-03, -3.7365e-03,  8.5698e-05,\n",
      "        -3.0588e-03, -3.5061e-03,  1.9884e-04,  2.6071e-03,  3.0334e-03,\n",
      "        -8.7142e-04, -2.4355e-03, -2.3563e-03, -2.7149e-03], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.self_attn.in_proj_weight tensor([[-0.0362, -0.0348,  0.1298,  ...,  0.1571, -0.0137,  0.0179],\n",
      "        [ 0.1267,  0.1199,  0.1281,  ..., -0.1279, -0.0771,  0.1638],\n",
      "        [ 0.1140, -0.1230, -0.0825,  ..., -0.0931,  0.1471,  0.1200],\n",
      "        ...,\n",
      "        [-0.1303, -0.0949, -0.0792,  ..., -0.0594,  0.0767,  0.0726],\n",
      "        [-0.0966, -0.0894, -0.1390,  ..., -0.0489, -0.0182,  0.1103],\n",
      "        [ 0.1339,  0.0241,  0.1020,  ..., -0.0154,  0.0491, -0.0084]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.self_attn.in_proj_bias tensor([ 6.0706e-03, -2.7891e-02, -9.6836e-04, -1.7127e-02,  2.9628e-02,\n",
      "         4.5103e-03,  1.4791e-02, -1.3881e-02,  1.8328e-02, -6.6143e-03,\n",
      "         2.0178e-02, -1.5221e-02,  2.0352e-02,  2.5896e-02, -2.2029e-02,\n",
      "         1.8242e-02, -2.8881e-02,  9.5794e-04,  2.9581e-02,  3.7461e-02,\n",
      "        -5.1741e-03,  1.1592e-02, -2.7229e-02, -2.2118e-02,  1.3505e-02,\n",
      "        -1.0933e-02, -3.2378e-02, -6.6934e-03, -3.0637e-03,  1.2784e-02,\n",
      "         1.5967e-02, -5.2933e-03,  2.2040e-02,  1.1438e-02,  4.3395e-03,\n",
      "        -1.2715e-02, -2.2867e-02,  1.1850e-02, -1.7706e-02,  5.8829e-03,\n",
      "         1.2429e-02,  1.3893e-02, -7.3759e-03, -1.4874e-03,  1.8906e-02,\n",
      "         2.2695e-02, -7.0194e-03, -4.2397e-03, -2.1556e-02,  4.2633e-03,\n",
      "         2.1968e-02,  1.3464e-02, -1.9764e-02, -2.5640e-02,  2.0209e-02,\n",
      "         3.1589e-02,  1.3672e-02,  1.0074e-02,  6.1888e-03, -2.7482e-02,\n",
      "         2.1305e-03, -3.0139e-02,  3.2456e-02,  5.8074e-02, -3.0349e-04,\n",
      "         2.7422e-02,  7.9938e-04,  8.1932e-03, -1.1314e-02, -5.4859e-03,\n",
      "        -1.0385e-02,  7.2028e-03, -1.7849e-02, -5.4097e-03, -1.0107e-02,\n",
      "         6.2331e-04, -9.3771e-03, -1.5698e-02, -3.0665e-03, -2.0487e-02,\n",
      "         1.1964e-03, -5.9235e-03, -1.1057e-02,  2.9368e-03, -1.0302e-02,\n",
      "         5.2474e-03,  1.9523e-02,  7.4737e-03, -1.6672e-02,  2.5389e-03,\n",
      "         1.4877e-02,  4.3215e-03, -1.0843e-03, -1.4121e-02,  1.0693e-02,\n",
      "        -9.9727e-03,  2.3161e-03, -2.3674e-02,  5.5987e-03, -6.6220e-03,\n",
      "        -3.1416e-03, -1.2868e-02,  1.3922e-02, -3.8807e-03, -5.9262e-03,\n",
      "        -1.0430e-02,  4.5607e-03, -1.7549e-04, -1.9305e-02,  4.0139e-03,\n",
      "         8.1475e-03, -6.7181e-03,  1.0378e-02,  2.2381e-02,  1.0480e-02,\n",
      "        -2.5297e-02, -3.1620e-03, -1.1689e-02,  5.5150e-03,  1.9544e-04,\n",
      "        -2.2373e-02,  2.6766e-03, -1.9274e-02,  5.8578e-03, -1.4887e-02,\n",
      "         1.3192e-03, -1.1639e-02,  2.9994e-03,  1.6135e-03,  1.4530e-03,\n",
      "         3.2518e-03, -2.4139e-03,  2.3765e-03, -5.3338e-03, -7.7065e-04,\n",
      "        -1.5180e-03,  9.5974e-04,  3.0322e-04, -3.7182e-03, -1.2029e-03,\n",
      "         9.4043e-04, -1.8572e-03,  5.3796e-04,  2.0669e-03,  5.1963e-03,\n",
      "         1.7546e-03, -2.7696e-03,  8.0867e-06,  7.2288e-04, -3.0709e-03,\n",
      "         1.0290e-03,  5.2415e-03,  2.0249e-03,  2.8241e-03,  3.3369e-03,\n",
      "        -2.0148e-03,  9.0575e-03,  2.4101e-03, -2.2625e-03,  5.7158e-03,\n",
      "        -4.6524e-03, -3.4252e-03, -8.7652e-04,  3.2502e-03, -8.5809e-03,\n",
      "         1.9249e-03,  1.6952e-03,  1.8927e-03,  4.8072e-04, -2.0207e-03,\n",
      "         6.1780e-03,  1.2677e-03,  7.5358e-03, -1.7824e-03,  9.3924e-04,\n",
      "         4.0451e-03, -3.3375e-03, -7.2005e-04,  6.8814e-04,  1.3960e-03,\n",
      "         7.9299e-04,  4.1642e-03, -4.0326e-03,  1.4623e-03, -5.2193e-03,\n",
      "        -9.3343e-04, -1.2319e-03,  1.9817e-03, -4.5389e-03, -4.8984e-04,\n",
      "         2.2669e-03,  1.2991e-04], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.self_attn.out_proj.weight tensor([[ 0.0268,  0.0270, -0.2088,  ...,  0.1598, -0.1896,  0.0323],\n",
      "        [ 0.1807,  0.1107, -0.0291,  ..., -0.0462,  0.0807, -0.0831],\n",
      "        [ 0.1609,  0.0811,  0.1872,  ...,  0.0532, -0.1137, -0.1901],\n",
      "        ...,\n",
      "        [ 0.1241,  0.1672,  0.1783,  ...,  0.0983, -0.0300,  0.1680],\n",
      "        [ 0.1534, -0.0553,  0.0072,  ...,  0.1228, -0.1474, -0.0909],\n",
      "        [ 0.1612,  0.2133, -0.1954,  ..., -0.0506, -0.1319, -0.0253]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.self_attn.out_proj.bias tensor([-1.7017e-03,  1.6206e-03,  4.1681e-03, -2.7724e-03, -4.6021e-03,\n",
      "        -2.2460e-03, -2.3578e-03, -2.0210e-03,  4.9028e-03, -1.7775e-03,\n",
      "         2.3902e-03, -6.9248e-04, -3.5314e-04, -5.4249e-03,  1.7387e-04,\n",
      "         3.3765e-03, -3.6894e-03, -1.1689e-03,  4.4102e-03, -5.6219e-03,\n",
      "         5.4440e-03,  1.6462e-03, -3.8728e-03,  2.7413e-03,  5.6775e-04,\n",
      "         3.3701e-03, -5.2986e-03,  2.2839e-03, -1.1432e-03, -4.9215e-03,\n",
      "        -2.7698e-03, -7.7033e-03, -2.2692e-03,  3.6248e-04,  4.6680e-03,\n",
      "        -5.5874e-04,  8.2995e-04,  2.2885e-03,  2.3041e-03,  1.6418e-03,\n",
      "         1.7335e-03,  1.8011e-03,  7.3203e-04, -3.3277e-03,  1.7605e-03,\n",
      "         1.6140e-03,  3.8368e-04, -2.2803e-03,  4.4950e-05, -4.8288e-03,\n",
      "         3.5179e-03, -2.9242e-03,  5.4994e-03,  2.9211e-03, -4.2773e-03,\n",
      "        -6.2993e-03, -1.5051e-03,  9.8252e-04,  3.6058e-03,  3.0172e-03,\n",
      "         9.4707e-04, -2.4005e-03, -2.7173e-03,  4.4978e-03], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.linear1.weight tensor([[ 0.0064,  0.0334, -0.0140,  ...,  0.0068,  0.0461, -0.0226],\n",
      "        [-0.0571,  0.0470,  0.0104,  ..., -0.0522, -0.0597,  0.0091],\n",
      "        [-0.0185,  0.0160, -0.0237,  ..., -0.0210, -0.0425, -0.0270],\n",
      "        ...,\n",
      "        [-0.0125,  0.0353,  0.0096,  ...,  0.0203,  0.0328, -0.0560],\n",
      "        [-0.0621, -0.0547, -0.0231,  ..., -0.0070, -0.0240, -0.0072],\n",
      "        [-0.0447,  0.0380, -0.0445,  ...,  0.0258, -0.0247,  0.0301]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.linear1.bias tensor([-0.0186,  0.0615, -0.1059,  ..., -0.0334,  0.0889, -0.0341],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.linear2.weight tensor([[-0.0233,  0.0158, -0.0350,  ...,  0.0043, -0.0284, -0.0189],\n",
      "        [-0.0057, -0.0398, -0.0590,  ...,  0.0356, -0.0424,  0.0152],\n",
      "        [ 0.0428, -0.0421,  0.0026,  ...,  0.0105, -0.0083,  0.0462],\n",
      "        ...,\n",
      "        [ 0.0429,  0.0126,  0.0119,  ...,  0.0348, -0.0024, -0.0541],\n",
      "        [-0.0048, -0.0353, -0.0018,  ...,  0.0204, -0.0036,  0.0025],\n",
      "        [ 0.0563,  0.0267,  0.0444,  ..., -0.0147, -0.0032,  0.0025]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.linear2.bias tensor([ 0.0097, -0.0172,  0.0100, -0.0099, -0.0003, -0.0178,  0.0137,  0.0220,\n",
      "         0.0102,  0.0080,  0.0143,  0.0129, -0.0121, -0.0189, -0.0081, -0.0001,\n",
      "         0.0181, -0.0146, -0.0050,  0.0005,  0.0036, -0.0199, -0.0217, -0.0195,\n",
      "        -0.0186, -0.0119,  0.0129,  0.0203, -0.0045, -0.0156, -0.0133, -0.0277,\n",
      "         0.0120, -0.0026,  0.0077,  0.0092,  0.0140,  0.0185,  0.0115, -0.0063,\n",
      "         0.0061, -0.0093, -0.0134,  0.0066, -0.0097, -0.0047,  0.0223, -0.0146,\n",
      "         0.0134, -0.0253, -0.0011,  0.0171,  0.0174,  0.0009,  0.0093, -0.0020,\n",
      "        -0.0118,  0.0118,  0.0136,  0.0249,  0.0151, -0.0087, -0.0036,  0.0129],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.norm1.weight tensor([1.0015, 1.0206, 1.0247, 1.0061, 1.0110, 0.9957, 1.0034, 1.0156, 1.0465,\n",
      "        0.9993, 1.0095, 1.0018, 1.0141, 1.0218, 1.0393, 1.0344, 1.0260, 1.0660,\n",
      "        1.0272, 1.0232, 1.0010, 1.0117, 1.0381, 0.9795, 0.9852, 0.9840, 1.0150,\n",
      "        0.9896, 0.9908, 0.9717, 0.9932, 0.9930, 0.9764, 1.0130, 0.9684, 1.0307,\n",
      "        1.0008, 0.9975, 0.9786, 0.9911, 0.9764, 0.9947, 0.9971, 1.0624, 0.9942,\n",
      "        1.0124, 0.9922, 0.9982, 0.9762, 1.0240, 0.9711, 1.0269, 0.9910, 1.0041,\n",
      "        0.9639, 1.0250, 1.0040, 0.9896, 1.0174, 1.0148, 0.9706, 1.0158, 0.9723,\n",
      "        0.9825], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.norm1.bias tensor([-0.0017,  0.0021,  0.0038, -0.0025, -0.0043, -0.0027, -0.0025, -0.0010,\n",
      "         0.0055, -0.0014,  0.0016, -0.0006,  0.0012, -0.0057,  0.0004,  0.0037,\n",
      "        -0.0034, -0.0027,  0.0046, -0.0055,  0.0053,  0.0022, -0.0041,  0.0038,\n",
      "         0.0005,  0.0035, -0.0039,  0.0006, -0.0014, -0.0052, -0.0033, -0.0080,\n",
      "        -0.0022,  0.0006,  0.0048,  0.0010,  0.0015,  0.0016,  0.0033,  0.0017,\n",
      "         0.0020,  0.0008,  0.0005, -0.0030,  0.0015,  0.0006,  0.0017, -0.0015,\n",
      "         0.0013, -0.0032,  0.0032, -0.0035,  0.0069,  0.0021, -0.0045, -0.0061,\n",
      "        -0.0008,  0.0009,  0.0033,  0.0031,  0.0006, -0.0027, -0.0021,  0.0046],\n",
      "       device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.norm2.weight tensor([0.9298, 0.9465, 0.9424, 0.9550, 0.9485, 0.9547, 0.9494, 0.9804, 0.9565,\n",
      "        0.9331, 0.9780, 0.9247, 0.9646, 0.9438, 0.9440, 0.9341, 0.9590, 0.9271,\n",
      "        0.9351, 0.9018, 0.9169, 0.9444, 0.9605, 0.9235, 0.9302, 0.9251, 0.9219,\n",
      "        0.9499, 0.9473, 0.9547, 0.9671, 0.9370, 0.9482, 0.9480, 0.9602, 0.9448,\n",
      "        0.9486, 0.9554, 0.9571, 0.9547, 0.9589, 0.9398, 0.9799, 0.9359, 0.9546,\n",
      "        0.9663, 0.9678, 0.9731, 0.9598, 0.9497, 0.9395, 0.9399, 0.9540, 0.9276,\n",
      "        0.9505, 0.9452, 0.9793, 0.9489, 0.9785, 0.9602, 0.9503, 0.9269, 0.9457,\n",
      "        0.9064], device='cuda:0')\n",
      "mainClassifier.transformer_encoder.layers.1.norm2.bias tensor([ 0.0022, -0.0030, -0.0025,  0.0088,  0.0038,  0.0061,  0.0030, -0.0012,\n",
      "        -0.0138,  0.0030,  0.0015,  0.0034, -0.0059,  0.0061,  0.0081,  0.0015,\n",
      "         0.0062,  0.0040,  0.0014,  0.0007,  0.0075,  0.0077,  0.0044,  0.0016,\n",
      "         0.0132,  0.0101, -0.0051,  0.0085, -0.0054,  0.0009,  0.0145,  0.0057,\n",
      "         0.0032, -0.0104, -0.0034, -0.0013, -0.0043,  0.0033, -0.0060,  0.0012,\n",
      "         0.0070, -0.0006,  0.0074, -0.0072, -0.0009, -0.0018,  0.0005, -0.0031,\n",
      "        -0.0010,  0.0061,  0.0031,  0.0068, -0.0034,  0.0157,  0.0080,  0.0015,\n",
      "        -0.0007, -0.0015,  0.0004,  0.0029,  0.0080, -0.0057,  0.0025, -0.0024],\n",
      "       device='cuda:0')\n",
      "mainClassifier.fc1.weight tensor([[-0.0593, -0.0657, -0.1047,  ..., -0.0390, -0.0484,  0.0757],\n",
      "        [ 0.1042, -0.1026,  0.0307,  ..., -0.0204,  0.0921, -0.1257],\n",
      "        [ 0.0942, -0.0214, -0.0201,  ..., -0.0244,  0.0645,  0.0715],\n",
      "        ...,\n",
      "        [-0.0633,  0.1242,  0.0214,  ..., -0.0784,  0.0482, -0.0250],\n",
      "        [ 0.0965,  0.0764, -0.0133,  ...,  0.0554, -0.0280, -0.1037],\n",
      "        [-0.1091, -0.0188, -0.1134,  ..., -0.0731, -0.0769,  0.0372]],\n",
      "       device='cuda:0')\n",
      "mainClassifier.fc1.bias tensor([ 9.9064e-02, -6.9040e-02,  6.4527e-02,  7.8709e-02, -4.4182e-02,\n",
      "        -6.5813e-02, -5.4474e-02,  1.0394e-01,  1.2921e-02, -2.2427e-02,\n",
      "        -5.5108e-02, -5.6838e-03, -8.6130e-02, -7.1655e-02, -1.1268e-01,\n",
      "         1.4225e-02,  9.2736e-02,  1.0014e-01, -3.6594e-02, -1.0684e-01,\n",
      "         5.6427e-02, -2.3551e-02, -1.2320e-02, -5.0438e-02,  3.8951e-02,\n",
      "         6.2702e-02, -1.2779e-02,  1.2732e-02, -5.1374e-02,  1.0697e-01,\n",
      "         3.8258e-02,  9.0302e-02,  7.5467e-02, -9.9150e-02,  1.3354e-01,\n",
      "         5.4914e-02,  4.8120e-02, -6.3933e-02, -2.4535e-02, -4.2452e-02,\n",
      "         1.1580e-01,  1.0166e-01, -9.8122e-02,  9.1601e-02,  1.0766e-02,\n",
      "         6.7976e-02, -2.6261e-03,  7.4646e-02, -9.7149e-02,  6.3160e-02,\n",
      "         6.2619e-03,  8.8780e-02, -6.8332e-02,  8.2454e-02,  6.4893e-02,\n",
      "        -6.3806e-02, -8.7767e-02,  1.0010e-01, -1.6471e-02,  5.8771e-02,\n",
      "         1.0485e-01,  3.4273e-02, -4.6074e-02,  1.1967e-01,  4.6788e-02,\n",
      "        -7.8478e-02, -8.8958e-03, -2.0212e-02, -8.8254e-02,  5.5288e-02,\n",
      "        -1.3594e-02, -8.6093e-02,  1.9766e-03,  9.6798e-02, -1.7091e-02,\n",
      "        -4.8829e-02, -5.6444e-02,  2.4424e-02, -1.1645e-01,  7.3208e-02,\n",
      "         4.0057e-02,  2.4640e-02, -1.7680e-02, -1.7462e-02, -7.3160e-02,\n",
      "        -4.2817e-02,  5.4324e-02, -3.3095e-02, -1.6696e-02,  1.0452e-01,\n",
      "        -8.7468e-02,  8.9353e-02,  3.7436e-02, -1.0302e-01, -2.1356e-03,\n",
      "        -7.2196e-02,  6.2796e-02,  1.8852e-02, -9.8913e-02,  4.8649e-02,\n",
      "        -7.4664e-02,  5.0947e-02,  1.0264e-01, -7.5465e-02,  5.1550e-02,\n",
      "        -9.2708e-02,  5.0197e-02, -4.0533e-02,  9.4287e-02, -1.6682e-02,\n",
      "         3.1189e-02, -1.1732e-01,  1.7083e-03,  1.2502e-01, -3.6031e-02,\n",
      "        -6.7537e-02,  7.7361e-02,  3.6213e-02, -1.0525e-01, -9.8878e-02,\n",
      "         2.3592e-02,  8.8269e-02,  2.1875e-02,  1.1112e-01, -9.7005e-02,\n",
      "        -7.0905e-02,  2.1810e-02,  1.1095e-01,  1.1069e-01, -1.0438e-01,\n",
      "        -4.7263e-02,  7.9291e-02,  6.3537e-02,  1.0777e-04,  9.4189e-02,\n",
      "        -4.4597e-02, -1.9521e-02,  2.8420e-02,  9.6944e-02, -1.1505e-01,\n",
      "         2.1577e-02, -6.1194e-02,  8.0284e-03,  9.8066e-02, -1.2097e-02,\n",
      "        -3.8943e-02, -9.0659e-02, -1.7880e-02,  7.6024e-02,  9.5723e-02,\n",
      "        -3.2628e-02, -2.0577e-02, -5.3022e-02,  1.2408e-01, -1.7661e-02,\n",
      "        -1.0119e-01, -8.4421e-02,  4.2060e-02, -1.2418e-01, -1.0340e-01,\n",
      "        -2.4945e-02, -1.1214e-01,  3.3312e-02, -9.8186e-02,  4.9572e-02,\n",
      "        -5.4149e-02,  5.2201e-02, -5.8424e-02, -3.9208e-02, -5.3886e-02,\n",
      "        -4.1462e-03,  1.0790e-01, -1.4638e-02, -1.0481e-01, -4.9910e-02,\n",
      "         6.7214e-02,  4.5756e-02, -9.6052e-02, -2.7986e-03,  6.1778e-02,\n",
      "         8.9155e-02, -4.0310e-03,  3.5264e-02,  3.9819e-02, -2.7869e-03,\n",
      "        -1.1416e-01,  7.1845e-02,  1.0464e-01,  7.4951e-02, -1.5587e-02,\n",
      "        -3.0482e-03, -3.8379e-02, -7.8290e-02, -1.0715e-01,  1.0243e-01,\n",
      "         6.3107e-02,  1.1197e-01,  1.7742e-02,  2.0138e-02,  1.1851e-01,\n",
      "        -2.8344e-02,  7.9118e-02, -6.1480e-03,  2.9579e-02,  1.1115e-01,\n",
      "         2.9723e-02,  9.8281e-02, -1.1136e-02,  4.9443e-02,  6.6335e-02,\n",
      "        -8.9357e-02,  7.6973e-02, -3.5758e-03, -1.9880e-02,  3.1043e-02,\n",
      "        -1.6751e-02,  2.0879e-02,  6.3946e-02,  1.9276e-02, -1.9681e-02,\n",
      "         9.9708e-02, -1.1780e-02, -8.9360e-02,  2.7979e-02, -1.1667e-01,\n",
      "        -3.0558e-02, -1.9329e-02, -6.6330e-02, -1.1856e-01,  6.1897e-02,\n",
      "         1.3858e-01, -9.9875e-02,  4.5964e-02,  1.2779e-01,  8.0956e-02,\n",
      "        -2.6139e-02, -6.1240e-02, -4.5635e-02, -1.1195e-01,  4.5299e-02,\n",
      "        -4.2232e-02, -3.5587e-02, -9.5190e-02, -7.9780e-02,  1.0689e-01,\n",
      "        -1.6001e-02,  1.0195e-02,  1.1841e-01,  3.8406e-02, -6.7997e-03,\n",
      "        -8.2286e-02,  8.5950e-02,  4.2241e-02, -6.9918e-02, -5.7892e-02,\n",
      "         8.9970e-02], device='cuda:0')\n",
      "mainClassifier.classifier.weight tensor([[ 4.5943e-03, -8.6278e-02,  7.4244e-02, -1.7096e-02, -9.9904e-02,\n",
      "          4.9517e-02, -6.3594e-02, -1.3224e-01, -1.5718e-02, -8.8000e-03,\n",
      "         -5.3350e-02, -1.2735e-01, -1.3010e-01, -1.2253e-01, -1.2033e-01,\n",
      "          2.0351e-02,  1.1524e-01, -6.1997e-02,  5.0359e-02, -5.4703e-02,\n",
      "         -2.1063e-02,  1.2739e-01,  6.5329e-02,  2.2135e-02, -1.3962e-01,\n",
      "          3.6135e-02,  6.3429e-02,  4.6108e-02, -1.1412e-01,  6.9976e-02,\n",
      "         -7.1101e-03, -1.5507e-02,  5.3153e-02, -6.0117e-02,  5.4444e-02,\n",
      "         -1.9779e-02,  4.6580e-02,  1.0916e-01,  1.6595e-02,  9.1701e-02,\n",
      "         -3.7339e-02, -2.0120e-02, -4.3718e-02,  2.4450e-02,  3.2832e-02,\n",
      "          1.7569e-02,  2.7135e-02,  1.9885e-02,  8.7487e-02,  4.5639e-02,\n",
      "         -7.9657e-02, -6.6854e-02, -8.9420e-02, -9.0012e-02,  2.3851e-02,\n",
      "          6.4988e-02, -7.8446e-02,  5.3533e-02, -6.8972e-02, -4.0184e-02,\n",
      "         -1.1859e-01,  1.7613e-02,  9.7767e-02, -7.2128e-02, -5.8916e-02,\n",
      "          3.2024e-03,  7.9470e-02,  8.9526e-02,  2.8688e-02, -4.4059e-02,\n",
      "          2.4615e-02, -5.4412e-02,  3.3645e-02,  6.4051e-02, -1.0584e-01,\n",
      "         -3.6211e-02, -7.5928e-02,  4.6333e-02, -7.1188e-02, -1.0473e-01,\n",
      "          7.0226e-02,  8.7092e-02,  1.0066e-01, -1.9695e-03, -3.3986e-02,\n",
      "         -5.1202e-02, -7.1838e-02,  8.4989e-02,  8.4229e-02, -3.6080e-02,\n",
      "         -1.0404e-01,  5.1116e-02, -1.1136e-01,  5.1268e-02,  1.2484e-01,\n",
      "          8.1854e-02, -7.2947e-02, -7.3991e-02, -7.1683e-02,  4.8850e-02,\n",
      "          1.9530e-02,  6.9499e-02, -9.5219e-02, -5.4693e-02, -1.0736e-01,\n",
      "         -1.9371e-02,  6.7956e-02, -9.2212e-02, -7.2916e-02,  2.0560e-02,\n",
      "          7.1888e-03, -1.0454e-01,  8.4800e-02,  2.7629e-02,  6.3686e-02,\n",
      "         -6.1623e-02, -8.2012e-02, -3.6227e-02,  8.8229e-02, -7.6773e-02,\n",
      "         -5.1955e-02, -9.5328e-03, -8.6134e-02, -6.1070e-02, -4.3889e-02,\n",
      "          1.2918e-02, -1.3433e-03, -3.8478e-02,  4.9658e-02, -1.3800e-02,\n",
      "          8.4852e-02, -1.5971e-01,  1.0659e-01,  6.1611e-02,  7.7728e-02,\n",
      "          1.0884e-01,  4.0017e-03,  5.2354e-02, -3.4338e-02,  5.8596e-02,\n",
      "          1.1274e-01, -2.7803e-02, -1.0685e-01,  1.4464e-03, -1.8965e-02,\n",
      "         -7.4315e-02,  6.6294e-02, -1.4711e-02, -2.3979e-03, -5.8108e-02,\n",
      "         -7.7275e-02,  5.2102e-02,  5.0696e-02,  7.6101e-02,  4.7545e-02,\n",
      "          7.2163e-02,  8.4420e-02, -5.2561e-02, -3.5579e-02,  1.0452e-01,\n",
      "          9.5631e-02,  6.0160e-02,  9.0720e-02,  1.3181e-02, -9.0246e-02,\n",
      "         -4.2541e-03,  2.4757e-02, -4.2664e-02, -3.3487e-03, -1.3465e-02,\n",
      "          8.0150e-02, -8.9354e-02,  2.0080e-02,  9.6986e-02, -4.4836e-02,\n",
      "          6.4707e-02, -5.6213e-02,  1.4153e-02, -1.0082e-01,  9.8086e-02,\n",
      "         -1.0936e-03,  8.0573e-02,  1.1963e-01,  6.3688e-02, -3.1102e-02,\n",
      "         -5.6254e-02, -1.2023e-01, -4.2778e-03, -1.0161e-01, -8.3180e-02,\n",
      "          1.1028e-01,  1.1072e-01, -9.0386e-02, -1.1550e-02,  4.5997e-02,\n",
      "         -1.2674e-01, -1.3776e-03, -2.5862e-02,  1.1760e-01, -3.9102e-02,\n",
      "         -1.1291e-01,  4.7810e-02, -6.6894e-02,  7.5995e-02,  9.5967e-03,\n",
      "         -1.4074e-02, -1.0512e-01,  1.0457e-01,  8.3041e-02,  8.1986e-02,\n",
      "          9.8716e-02,  2.3151e-02, -1.0980e-01, -4.8461e-02,  4.5754e-02,\n",
      "         -7.7585e-02,  4.3178e-02,  4.3691e-02, -1.4427e-01,  1.0919e-01,\n",
      "         -4.6308e-02, -4.0762e-02, -3.1277e-02,  2.9553e-02,  6.2251e-02,\n",
      "         -9.2373e-02,  4.7536e-02, -6.3851e-02, -2.7191e-02,  1.5509e-02,\n",
      "          7.4728e-02,  1.2375e-02,  1.6705e-02, -3.0045e-02,  1.3166e-02,\n",
      "         -5.9186e-02,  9.3820e-02,  1.0320e-01, -4.6583e-02,  3.3280e-02,\n",
      "         -1.8752e-02, -3.5992e-02,  7.7995e-02,  8.5664e-03,  2.5178e-02,\n",
      "         -7.4446e-02, -5.9360e-02, -3.0139e-02,  7.3872e-02,  7.7852e-02,\n",
      "         -1.3578e-01, -5.5205e-02, -3.7571e-02,  1.3137e-01, -6.3534e-02,\n",
      "          1.3567e-02],\n",
      "        [ 6.3388e-02, -9.8778e-02, -8.8050e-02,  1.0311e-01, -3.6945e-02,\n",
      "          2.8359e-02, -7.6134e-02, -6.9468e-02, -5.0407e-02,  9.2547e-02,\n",
      "          9.1178e-02, -1.2519e-01, -1.1133e-01, -1.3570e-01, -8.5240e-02,\n",
      "          1.1416e-02,  1.2352e-01, -4.4807e-02,  1.3085e-01,  8.0854e-02,\n",
      "          8.6330e-02,  9.9292e-02, -7.3573e-02,  7.4822e-02, -8.4067e-02,\n",
      "         -6.4687e-02, -2.3092e-02,  6.1910e-02, -9.0876e-02,  1.0121e-01,\n",
      "         -8.2221e-02, -8.7780e-02, -5.6724e-02, -1.1100e-01,  6.9752e-02,\n",
      "         -1.3510e-02, -5.2241e-02,  1.1797e-01,  6.3437e-02,  1.1523e-01,\n",
      "          1.9360e-02,  6.1714e-02,  1.1639e-02,  7.0263e-02,  7.0960e-02,\n",
      "         -1.8959e-03, -5.3366e-02,  2.7506e-02,  9.6470e-02, -4.0337e-02,\n",
      "         -5.9288e-02,  4.1553e-03,  2.2419e-02,  3.3525e-03,  4.6792e-02,\n",
      "         -3.9664e-02,  3.9328e-02,  9.5340e-02, -1.0680e-01, -1.5277e-02,\n",
      "         -1.3074e-01,  7.6923e-02, -4.4528e-02,  7.3530e-02, -4.8946e-02,\n",
      "          6.4385e-02, -1.2685e-02, -4.1432e-02,  3.3197e-02,  1.1272e-02,\n",
      "         -2.6440e-02,  4.4218e-02,  9.6161e-02, -6.7595e-02, -7.7853e-02,\n",
      "          5.9094e-02, -1.0813e-01,  3.1673e-02, -1.1789e-01, -1.9315e-03,\n",
      "          3.4816e-02, -2.3773e-02,  2.7812e-02,  3.6431e-02, -6.8238e-02,\n",
      "         -8.1343e-02, -6.5762e-02, -6.4400e-02, -6.0362e-03, -8.1401e-02,\n",
      "         -2.2207e-02,  4.2836e-02, -1.2847e-01,  4.9461e-02,  7.0754e-02,\n",
      "         -1.6088e-04, -1.8495e-02, -1.0635e-01, -8.9264e-02,  8.9575e-02,\n",
      "         -9.1946e-02,  6.3123e-02, -7.8135e-02,  7.8561e-02, -2.6335e-02,\n",
      "          7.3163e-02, -1.7233e-02, -4.3592e-02,  6.8386e-02, -4.1303e-02,\n",
      "         -5.8599e-02,  2.1540e-02,  2.6024e-02,  2.5403e-02,  7.0409e-02,\n",
      "         -9.2135e-03, -7.6220e-02,  9.3044e-02, -6.5707e-03, -1.0595e-01,\n",
      "         -5.8074e-02,  8.7351e-02, -1.2661e-02,  5.4873e-02,  6.3494e-02,\n",
      "          1.0049e-01,  6.5842e-02,  4.3805e-02,  2.3803e-02, -7.8443e-02,\n",
      "          2.4654e-02, -1.0629e-01,  1.1930e-01, -9.4927e-02, -8.8843e-02,\n",
      "          6.8807e-02,  2.1998e-02, -1.8511e-02,  5.4870e-02,  3.4269e-02,\n",
      "         -2.0473e-02,  8.9644e-02, -4.2632e-02, -1.0936e-04,  9.7677e-04,\n",
      "         -1.2413e-01,  6.6507e-02, -4.0963e-03,  4.1912e-02,  5.1313e-02,\n",
      "         -1.0174e-02,  3.1875e-02,  4.8801e-02,  8.7523e-02,  8.2199e-04,\n",
      "          5.7616e-02,  5.7514e-03, -1.1806e-01, -5.8391e-02,  1.1148e-01,\n",
      "          1.7151e-02, -4.9174e-02,  1.8403e-02,  8.6084e-02,  1.3135e-02,\n",
      "         -2.0253e-03,  1.4068e-02,  8.6034e-02,  5.2635e-02, -3.2768e-02,\n",
      "         -7.2572e-02,  6.6762e-02,  9.9925e-02, -1.0938e-01, -8.5825e-02,\n",
      "         -2.5864e-02,  8.3911e-03, -9.3323e-02, -3.3533e-02,  1.0728e-01,\n",
      "          1.1286e-01,  1.0302e-01,  1.3248e-01,  1.1437e-01, -1.0056e-02,\n",
      "         -4.8024e-02, -1.0128e-01, -5.6780e-02,  4.6813e-03, -3.4912e-02,\n",
      "          8.6544e-03,  7.9664e-02, -7.5923e-02, -1.0260e-01, -4.9642e-02,\n",
      "         -1.1180e-01, -3.0926e-03,  2.5075e-02,  4.0865e-02, -9.3069e-03,\n",
      "         -1.2889e-01,  1.0796e-01,  2.5844e-02, -1.9921e-03, -2.1683e-02,\n",
      "          1.9584e-02, -7.0587e-02,  6.4965e-02,  2.0061e-02,  3.2546e-02,\n",
      "          1.0634e-01, -5.1724e-02, -1.1949e-01,  5.6750e-02,  4.2002e-02,\n",
      "          7.9633e-02, -1.5189e-02,  7.9409e-02, -1.2274e-01, -2.1049e-02,\n",
      "         -9.2121e-02,  7.4768e-03, -5.2575e-02, -4.0239e-03, -5.4603e-02,\n",
      "         -2.4421e-02,  4.5361e-02, -1.5186e-02, -4.8852e-02,  1.0503e-01,\n",
      "          1.1136e-01,  5.9420e-02,  1.1452e-01, -3.0098e-02, -8.3365e-02,\n",
      "          1.7523e-02, -1.0205e-02,  9.4390e-02, -5.3851e-02,  7.6911e-02,\n",
      "         -8.3654e-02, -3.7465e-02, -4.1615e-02, -2.9390e-02, -1.7199e-02,\n",
      "         -4.4080e-03, -5.9936e-02,  9.2738e-03, -8.8639e-02, -9.2587e-02,\n",
      "         -8.8626e-02,  7.1150e-02, -1.0133e-01,  4.6789e-02, -6.5107e-02,\n",
      "         -3.3583e-02]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mainClassifier.classifier.bias tensor([-0.0600, -0.0126], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for name, param in combined_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ee6ca03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 40 of 42: loss 1.7775783538818366\n",
      "0.7824773413897281 0.8516992790937178\n"
     ]
    }
   ],
   "source": [
    "# load combined model weights\n",
    "checkpoint = torch.load('checkpoints/combined_v1_3_valdscloss1.395304_valf10.851699.model', map_location=DEVICE)\n",
    "combined_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "val_loss, val_cls_loss, val_dsc_loss, val_acc, val_true, val_pred, val_dsc_pred_logits, val_dsc_pred_probs = evaluate_adversarial(\n",
    "    combined_model = combined_model, \n",
    "    cls_loss_fn = cls_loss_fn, \n",
    "    dsc_loss_fn = dsc_loss_fn, \n",
    "    val_dataloader = val_dataloader,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "val_f1 = f1_score(val_true, val_pred)\n",
    "print(val_acc, val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1c7ff694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n",
      "C:\\Users\\haris\\AppData\\Local\\Temp/ipykernel_21716/171138243.py:22: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n",
      "  axs[row, col].set_xticks([])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "num_rows = 3\n",
    "num_cols = 10\n",
    "# use the same IDs as previously; don't sample again\n",
    "# pick_ids = np.random.choice(np.arange(len(val_dsc_pred_probs)), size=num_rows*num_cols, replace=False)\n",
    "demogs = ['Hispanic', 'Other', 'White', 'AA']\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(16,5))\n",
    "for i in range(num_rows*num_cols):\n",
    "    \n",
    "    # determine location in plot\n",
    "    row = i // num_cols\n",
    "    col = i % num_cols\n",
    "    \n",
    "    # grab probabilities and logits\n",
    "    example_id = pick_ids[i]\n",
    "    val_dsc_pred_probs_i = val_dsc_pred_probs[example_id:example_id+1]\n",
    "    val_dsc_pred_logits_i = val_dsc_pred_logits[example_id:example_id+1]\n",
    "    \n",
    "    # plot in the correct place\n",
    "    axs[row, col].bar(demogs, val_dsc_pred_probs_i.squeeze())\n",
    "    axs[row, col].set_xticks([])\n",
    "    axs[row, col].set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac355513",
   "metadata": {},
   "source": [
    "# Evaluate adversarially trained network on `demog_dev_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "dba7b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 150 of 159: loss 0.23937173187732697\n"
     ]
    }
   ],
   "source": [
    "demog_loss, demog_acc, demog_true, demog_pred, prot_attr = evaluate_with_protected_attribute(\n",
    "    model = combined_model.mainClassifier, \n",
    "    loss_fn = cls_loss_fn, \n",
    "    val_dataloader = demog_dev_dataloader,\n",
    "    idx_to_class = train_dataset.idx_to_class,\n",
    "    idx_to_prot_attr = demog_dev_dataset.idx_to_prot_attr,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "demog_true_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_true])\n",
    "demog_pred_numeric = np.array([1 if x == 'OFF' else 0 for x in demog_pred])\n",
    "demog_f1 = f1_score(1-demog_true_numeric, 1-demog_pred_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e3973856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# report FPR for different demographic groups in demog_dev_df:\n",
    "demog_dev_df_pred = pd.DataFrame({\n",
    "    'is_offensive_true': demog_true_numeric,\n",
    "    'is_offensive_pred': demog_pred_numeric,\n",
    "    'demographic': prot_attr\n",
    "})\n",
    "demog_fpr  = {\n",
    "    demog: fpr(\n",
    "        y_true = demog_dev_df_pred[demog_dev_df_pred['demographic'] == demog]['is_offensive_true'],\n",
    "        y_pred = demog_dev_df_pred[demog_dev_df_pred['demographic'] == demog]['is_offensive_pred']\n",
    "    ) for demog in demog_dev_df_pred['demographic'].unique()\n",
    "}\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.title('FPR per demographic group')\n",
    "plt.bar(demog_fpr.keys(), demog_fpr.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3c04015e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([0.1038961038961039, 0.12238805970149254, 0.22590361445783133, 0.0058823529411764705])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demog_fpr.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fe71b394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07807563870329791"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(list(demog_fpr.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "d7692185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='demographic'>"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# side by side\n",
    "demogs = list(demog_fpr.keys())\n",
    "demog_fpr_comparison_df = pd.DataFrame({\n",
    "    'demographic': demogs,\n",
    "    'no-adv': [demog_fpr_before[demog] for demog in demogs],\n",
    "    'adv': [demog_fpr[demog] for demog in demogs]\n",
    "})\n",
    "demog_fpr_comparison_df.plot(x = 'demographic', y = ['no-adv', 'adv'], kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bda2c9a",
   "metadata": {},
   "source": [
    "# Predict on `test.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845471a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('civility_data/test.tsv', sep='\\t')\n",
    "test_df['dummy_label'] = 'NOT'\n",
    "\n",
    "test_dataset = TextClassifierInferenceDataset(\n",
    "    train_dataset = train_dataset,\n",
    "    df = test_df,\n",
    "    source_column = 'text', \n",
    "    target_column = 'dummy_label',\n",
    ")\n",
    "\n",
    "test_dataloader   = get_inference_loader(\n",
    "    dataset = test_dataset, \n",
    "    train_dataset = train_dataset,\n",
    "    batch_size = 32\n",
    ")\n",
    "\n",
    "val_loss, val_cls_loss, val_dsc_loss, val_acc, val_true, val_pred, val_dsc_pred_logits, val_dsc_pred_probs = evaluate_adversarial(\n",
    "    combined_model = combined_model, \n",
    "    cls_loss_fn = cls_loss_fn, \n",
    "    dsc_loss_fn = dsc_loss_fn, \n",
    "    val_dataloader = test_dataloader,\n",
    "    DEVICE = DEVICE\n",
    ")\n",
    "\n",
    "val_pred_str = [train_dataset.idx_to_class[x] for x in val_pred]\n",
    "\n",
    "test_df['label'] = val_pred_str\n",
    "\n",
    "test_df[['text', 'label']].to_csv('IgnatiusHarisSetiawan_Widjaja_advanced.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd80c024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<SOS>',\n",
       " 'you',\n",
       " 'know',\n",
       " 'what',\n",
       " 'i',\n",
       " 'hate',\n",
       " '?',\n",
       " '<UNK>',\n",
       " '.',\n",
       " '<EOS>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>',\n",
       " '<PAD>']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "for i, (src, cls_tgt) in enumerate(test_dataloader):\n",
    "\n",
    "    # forward pass, obtaining output logits of both heads\n",
    "    src = src.to(DEVICE)\n",
    "    cls_tgt = cls_tgt.to(DEVICE)\n",
    "    \n",
    "    break\n",
    "    \n",
    "i = 2\n",
    "token_ids = src.cpu().detach().numpy().T[i].tolist()\n",
    "token_str = [train_dataset.vocab.itos[t] for t in token_ids]\n",
    "token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a87ccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp_ethics",
   "language": "python",
   "name": "comp_ethics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
